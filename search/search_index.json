{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Memlayer","text":"<p>The plug-and-play memory layer for smart, contextual agents</p> <p>Memlayer adds persistent, intelligent memory to any LLM, enabling agents that recall context across conversations, extract structured knowledge, and surface relevant information when it matters.</p> <p>&lt;100ms Fast Search \u2022 Noise-Aware Memory Gate \u2022 Multi-Tier Retrieval Modes \u2022 100% Local \u2022 Zero Config</p> <p> </p>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub: github.com/divagr18/memlayer</li> <li>PyPI: pypi.org/project/memlayer</li> </ul>"},{"location":"#quick-start","title":"Quick start","text":"<p>Install:</p> <pre><code>pip install memlayer\n</code></pre> <p>Basic usage:</p> <pre><code>from memlayer.wrappers.openai import OpenAI\n\nclient = OpenAI(model=\"gpt-4.1-mini\", storage_path=\"./memories\", user_id=\"user_123\")\nclient.chat([{\"role\": \"user\", \"content\": \"My name is Alice and I work at TechCorp\"}])\nresponse = client.chat([{\"role\": \"user\", \"content\": \"Where do I work?\"}])\n# -&gt; \"You work at TechCorp.\"\n</code></pre> <p>Memlayer automatically filters, extracts, stores and retrieves relevant memories, no manual prompts required.</p>"},{"location":"#what-makes-memlayer-different","title":"What makes Memlayer different","text":"<ul> <li>Selective long-term memory \u2014 we only store what matters, not every chat line.  </li> <li>Hybrid storage \u2014 semantic vectors for recall plus a lightweight knowledge graph for relationships and updates.  </li> <li>Noise-aware gate \u2014 cheap salience checks (prototype embeddings + TF-IDF) keep the store clean.  </li> <li>Multi-tier retrieval \u2014 Fast / Balanced / Deep modes so you can trade latency for depth.  </li> <li>Offline-first \u2014 runs locally with no external services required.  </li> <li>Zero config \u2014 drop it into your app and start remembering.</li> </ul>"},{"location":"#explore-the-docs","title":"Explore the docs","text":"<ul> <li>Basics &amp; Quickstart \u2014 architecture and getting started</li> <li>Search Tiers &amp; Modes \u2014 how retrieval and salience modes work</li> <li>API Reference \u2014 full method docs and examples</li> <li>Providers \u2014 OpenAI, Claude, Gemini, Ollama, and local model tips</li> <li>Storage Backends \u2014 ChromaDB + NetworkX details</li> </ul> <p>Or open the sidebar to browse everything.</p>"},{"location":"#examples-development","title":"Examples &amp; development","text":"<p>See <code>examples/</code> for runnable demos (basics, search tiers, provider samples). Clone and run locally:</p> <pre><code>git clone https://github.com/divagr18/memlayer.git\ncd memlayer\npip install -e .\npython examples/01_basics/getting_started.py\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Bug reports, PRs, and documentation fixes are welcome. See the repo <code>CONTRIBUTING.md</code> for guidelines.</p> <p>If you want a slightly shorter or more technical landing page (or want the badges moved here), tell me which tone you prefer and I\u2019ll adapt it.</p>"},{"location":"API_REFERENCE/","title":"Memlayer API Reference","text":"<p>Complete reference for all public methods and their parameters.</p>"},{"location":"API_REFERENCE/#client-initialization","title":"Client Initialization","text":"<p>All wrapper classes (<code>OpenAI</code>, <code>Claude</code>, <code>Gemini</code>, <code>Ollama</code>) share these common initialization parameters:</p> <pre><code>from memlayer.wrappers.openai import OpenAI\nfrom memlayer.wrappers.claude import Claude\nfrom memlayer.wrappers.gemini import Gemini\nfrom memlayer.wrappers.ollama import Ollama\n</code></pre>"},{"location":"API_REFERENCE/#common-parameters","title":"Common Parameters","text":"Parameter Type Default Description <code>api_key</code> <code>str</code> <code>None</code> API key (read from env if not provided) <code>model</code> <code>str</code> varies Model name/identifier <code>user_id</code> <code>str</code> <code>\"default_user\"</code> User identifier for memory isolation <code>operation_mode</code> <code>str</code> <code>\"online\"</code> <code>\"online\"</code>, <code>\"local\"</code>, or <code>\"lightweight\"</code> <code>chroma_dir</code> <code>str</code> <code>\"./chroma_db\"</code> Path to ChromaDB storage directory <code>networkx_path</code> <code>str</code> <code>\"./knowledge_graph.pkl\"</code> Path to NetworkX graph file <code>salience_threshold</code> <code>float</code> <code>0.5</code> Threshold for content filtering (0.0-1.0) <code>embedding_model</code> <code>str</code> varies Embedding model name <code>max_search_results</code> <code>int</code> <code>5</code> Maximum search results to return <code>search_tier</code> <code>str</code> <code>\"balanced\"</code> <code>\"fast\"</code>, <code>\"balanced\"</code>, or <code>\"deep\"</code> <code>curation_interval</code> <code>int</code> <code>3600</code> Curation check interval in seconds <code>temperature</code> <code>float</code> <code>0.7</code> LLM temperature <code>max_tokens</code> <code>int</code> <code>4096</code> Maximum tokens in response"},{"location":"API_REFERENCE/#provider-specific-parameters","title":"Provider-Specific Parameters","text":"<p>OpenAI:</p> <pre><code>client = OpenAI(\n    api_key=\"sk-...\",\n    model=\"gpt-4.1-mini\",\n    user_id=\"alice\"\n)\n</code></pre> <p>Claude:</p> <pre><code>client = Claude(\n    api_key=\"sk-ant-...\",\n    model=\"claude-4-sonnet\",\n    user_id=\"alice\"\n)\n</code></pre> <p>Gemini:</p> <pre><code>client = Gemini(\n    api_key=\"AIza...\",\n    model=\"gemini-2.5-flash\",\n    user_id=\"alice\"\n)\n</code></pre> <p>Ollama:</p> <pre><code>client = Ollama(\n    model=\"llama3.2\",\n    host=\"http://localhost:11434\",  # Additional parameter\n    user_id=\"alice\",\n    operation_mode=\"local\"\n)\n</code></pre>"},{"location":"API_REFERENCE/#core-methods","title":"Core Methods","text":""},{"location":"API_REFERENCE/#chat","title":"<code>chat()</code>","text":"<p>Send a chat completion request with memory capabilities.</p> <p>Signature:</p> <pre><code>def chat(\n    messages: List[Dict[str, str]],\n    stream: bool = False,\n    **kwargs\n) -&gt; str | Generator[str, None, None]\n</code></pre> <p>Parameters: | Parameter | Type | Required | Description | |-----------|------|----------|-------------| | <code>messages</code> | <code>List[Dict[str, str]]</code> | \u2705 | List of message dicts with <code>role</code> and <code>content</code> | | <code>stream</code> | <code>bool</code> | \u274c | If <code>True</code>, returns generator yielding chunks | | <code>**kwargs</code> | <code>Any</code> | \u274c | Additional provider-specific arguments |</p> <p>Returns: - If <code>stream=False</code>: <code>str</code> - Complete response text - If <code>stream=True</code>: <code>Generator[str, None, None]</code> - Generator yielding response chunks</p> <p>Example:</p> <pre><code># Non-streaming\nresponse = client.chat([\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n])\n\n# Streaming\nfor chunk in client.chat([\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n], stream=True):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"API_REFERENCE/#update_from_text","title":"<code>update_from_text()</code>","text":"<p>Directly ingest text into memory without conversational interaction.</p> <p>Signature:</p> <pre><code>def update_from_text(text_block: str) -&gt; None\n</code></pre> <p>Parameters: | Parameter | Type | Required | Description | |-----------|------|----------|-------------| | <code>text_block</code> | <code>str</code> | \u2705 | Text content to analyze and store |</p> <p>Returns: <code>None</code> (runs asynchronously in background)</p> <p>Example:</p> <pre><code>client.update_from_text(\"\"\"\nMeeting notes from Nov 15:\n- Q4 deadline: December 20th\n- New team member: Bob\n- Budget increased 15%\n\"\"\")\n</code></pre>"},{"location":"API_REFERENCE/#synthesize_answer","title":"<code>synthesize_answer()</code>","text":"<p>Generate a memory-grounded answer to a specific question.</p> <p>Signature:</p> <pre><code>def synthesize_answer(\n    question: str,\n    return_object: bool = False\n) -&gt; str | AnswerObject\n</code></pre> <p>Parameters: | Parameter | Type | Required | Description | |-----------|------|----------|-------------| | <code>question</code> | <code>str</code> | \u2705 | Question to answer | | <code>return_object</code> | <code>bool</code> | \u274c | If <code>True</code>, returns detailed <code>AnswerObject</code> |</p> <p>Returns: - If <code>return_object=False</code>: <code>str</code> - Answer text - If <code>return_object=True</code>: <code>AnswerObject</code> with fields:   - <code>answer: str</code> - The synthesized answer   - <code>sources: List[str]</code> - Source facts used   - <code>confidence: float</code> - Confidence score (0.0-1.0)</p> <p>Example:</p> <pre><code># Simple answer\nanswer = client.synthesize_answer(\"What is the Q4 deadline?\")\n\n# Detailed answer with sources\nanswer_obj = client.synthesize_answer(\n    \"What is the Q4 deadline?\",\n    return_object=True\n)\nprint(f\"Answer: {answer_obj.answer}\")\nprint(f\"Sources: {answer_obj.sources}\")\nprint(f\"Confidence: {answer_obj.confidence}\")\n</code></pre>"},{"location":"API_REFERENCE/#analyze_and_extract_knowledge","title":"<code>analyze_and_extract_knowledge()</code>","text":"<p>Extract structured knowledge from text (internal method, but can be called directly).</p> <p>Signature:</p> <pre><code>def analyze_and_extract_knowledge(text: str) -&gt; Dict[str, List[Dict]]\n</code></pre> <p>Parameters: | Parameter | Type | Required | Description | |-----------|------|----------|-------------| | <code>text</code> | <code>str</code> | \u2705 | Text to analyze |</p> <p>Returns: <code>Dict</code> with keys: - <code>facts</code>: <code>List[Dict]</code> - Extracted facts with <code>fact</code>, <code>importance_score</code>, <code>expiration_date</code> - <code>entities</code>: <code>List[Dict]</code> - Entities with <code>name</code> and <code>type</code> - <code>relationships</code>: <code>List[Dict]</code> - Relationships with <code>subject</code>, <code>predicate</code>, <code>object</code></p> <p>Example:</p> <pre><code>knowledge = client.analyze_and_extract_knowledge(\n    \"Alice works at TechCorp as a Senior Engineer\"\n)\n\nprint(knowledge[\"facts\"])\n# [{\"fact\": \"Alice works at TechCorp\", \"importance_score\": 0.9, ...}]\n\nprint(knowledge[\"entities\"])\n# [{\"name\": \"Alice\", \"type\": \"Person\"}, {\"name\": \"TechCorp\", \"type\": \"Organization\"}]\n\nprint(knowledge[\"relationships\"])\n# [{\"subject\": \"Alice\", \"predicate\": \"works at\", \"object\": \"TechCorp\"}]\n</code></pre>"},{"location":"API_REFERENCE/#search-service","title":"Search Service","text":"<p>Access via <code>client.search_service</code>.</p>"},{"location":"API_REFERENCE/#search","title":"<code>search()</code>","text":"<p>Search the knowledge graph and vector store.</p> <p>Signature:</p> <pre><code>def search(\n    query: str,\n    user_id: str,\n    search_tier: str = \"balanced\",\n    llm_client: Optional[Any] = None\n) -&gt; Dict[str, Any]\n</code></pre> <p>Parameters: | Parameter | Type | Required | Description | |-----------|------|----------|-------------| | <code>query</code> | <code>str</code> | \u2705 | Search query | | <code>user_id</code> | <code>str</code> | \u2705 | User ID for memory isolation | | <code>search_tier</code> | <code>str</code> | \u274c | <code>\"fast\"</code>, <code>\"balanced\"</code>, or <code>\"deep\"</code> | | <code>llm_client</code> | <code>Any</code> | \u274c | LLM client for entity extraction in deep search |</p> <p>Returns: <code>Dict</code> with keys: - <code>result</code>: <code>str</code> - Formatted search results - <code>trace</code>: <code>Trace</code> - Observability trace object</p> <p>Example:</p> <pre><code>search_output = client.search_service.search(\n    query=\"What do I know about Alice?\",\n    user_id=\"user123\",\n    search_tier=\"deep\",\n    llm_client=client\n)\n\nprint(search_output[\"result\"])\nprint(f\"Search took: {search_output['trace'].total_time_ms}ms\")\n</code></pre>"},{"location":"API_REFERENCE/#consolidation-service","title":"Consolidation Service","text":"<p>Access via <code>client.consolidation_service</code>.</p>"},{"location":"API_REFERENCE/#consolidate","title":"<code>consolidate()</code>","text":"<p>Extract and store knowledge from text (runs in background thread).</p> <p>Signature:</p> <pre><code>def consolidate(text: str, user_id: str) -&gt; None\n</code></pre> <p>Parameters: | Parameter | Type | Required | Description | |-----------|------|----------|-------------| | <code>text</code> | <code>str</code> | \u2705 | Text to consolidate | | <code>user_id</code> | <code>str</code> | \u2705 | User ID for memory isolation |</p> <p>Returns: <code>None</code> (runs asynchronously)</p> <p>Example:</p> <pre><code>client.consolidation_service.consolidate(\n    \"Alice mentioned the project deadline is next Friday\",\n    user_id=\"user123\"\n)\n</code></pre>"},{"location":"API_REFERENCE/#graph-storage","title":"Graph Storage","text":"<p>Access via <code>client.graph_storage</code>.</p>"},{"location":"API_REFERENCE/#add_task","title":"<code>add_task()</code>","text":"<p>Schedule a task with a due date.</p> <p>Signature:</p> <pre><code>def add_task(\n    description: str,\n    due_timestamp: float,\n    user_id: str\n) -&gt; str\n</code></pre> <p>Parameters: | Parameter | Type | Required | Description | |-----------|------|----------|-------------| | <code>description</code> | <code>str</code> | \u2705 | Task description | | <code>due_timestamp</code> | <code>float</code> | \u2705 | Unix timestamp when task is due | | <code>user_id</code> | <code>str</code> | \u2705 | User ID for task ownership |</p> <p>Returns: <code>str</code> - Task ID</p> <p>Example:</p> <pre><code>from datetime import datetime, timedelta\n\ndue_time = (datetime.now() + timedelta(days=7)).timestamp()\ntask_id = client.graph_storage.add_task(\n    description=\"Review project proposal\",\n    due_timestamp=due_time,\n    user_id=\"alice\"\n)\n</code></pre>"},{"location":"API_REFERENCE/#get_entity_subgraph","title":"<code>get_entity_subgraph()</code>","text":"<p>Get entity and its relationships from the graph.</p> <p>Signature:</p> <pre><code>def get_entity_subgraph(\n    entity_name: str,\n    user_id: str,\n    max_depth: int = 2\n) -&gt; Dict[str, Any]\n</code></pre> <p>Parameters: | Parameter | Type | Required | Description | |-----------|------|----------|-------------| | <code>entity_name</code> | <code>str</code> | \u2705 | Entity name to query | | <code>user_id</code> | <code>str</code> | \u2705 | User ID for isolation | | <code>max_depth</code> | <code>int</code> | \u274c | Maximum traversal depth |</p> <p>Returns: <code>Dict</code> with entity data and relationships</p> <p>Example:</p> <pre><code>subgraph = client.graph_storage.get_entity_subgraph(\n    entity_name=\"Alice\",\n    user_id=\"user123\",\n    max_depth=2\n)\n</code></pre>"},{"location":"API_REFERENCE/#vector-storage-chromadb","title":"Vector Storage (ChromaDB)","text":"<p>Access via <code>client.chroma_storage</code>.</p>"},{"location":"API_REFERENCE/#add_facts","title":"<code>add_facts()</code>","text":"<p>Add facts to vector store with embeddings.</p> <p>Signature:</p> <pre><code>def add_facts(\n    facts: List[str],\n    metadata_list: List[Dict],\n    user_id: str\n) -&gt; None\n</code></pre> <p>Parameters: | Parameter | Type | Required | Description | |-----------|------|----------|-------------| | <code>facts</code> | <code>List[str]</code> | \u2705 | List of fact strings | | <code>metadata_list</code> | <code>List[Dict]</code> | \u2705 | Metadata for each fact | | <code>user_id</code> | <code>str</code> | \u2705 | User ID for isolation |</p> <p>Returns: <code>None</code></p>"},{"location":"API_REFERENCE/#search_facts","title":"<code>search_facts()</code>","text":"<p>Search for similar facts using vector similarity.</p> <p>Signature:</p> <pre><code>def search_facts(\n    query: str,\n    user_id: str,\n    n_results: int = 5\n) -&gt; Dict[str, Any]\n</code></pre> <p>Parameters: | Parameter | Type | Required | Description | |-----------|------|----------|-------------| | <code>query</code> | <code>str</code> | \u2705 | Search query | | <code>user_id</code> | <code>str</code> | \u2705 | User ID for isolation | | <code>n_results</code> | <code>int</code> | \u274c | Maximum results to return |</p> <p>Returns: <code>Dict</code> with search results</p>"},{"location":"API_REFERENCE/#observability","title":"Observability","text":""},{"location":"API_REFERENCE/#trace-object","title":"Trace Object","text":"<p>Returned by search operations for performance monitoring.</p> <p>Attributes: - <code>total_time_ms</code>: <code>float</code> - Total search time in milliseconds - <code>vector_search_time_ms</code>: <code>float</code> - Time spent in vector search - <code>graph_traversal_time_ms</code>: <code>float</code> - Time spent in graph traversal - <code>entity_extraction_time_ms</code>: <code>float</code> - Time spent extracting entities - <code>search_tier</code>: <code>str</code> - Search tier used - <code>results_count</code>: <code>int</code> - Number of results returned</p> <p>Example:</p> <pre><code>output = client.search_service.search(query=\"...\", user_id=\"...\")\ntrace = output[\"trace\"]\n\nprint(f\"Total: {trace.total_time_ms}ms\")\nprint(f\"Vector search: {trace.vector_search_time_ms}ms\")\nprint(f\"Graph traversal: {trace.graph_traversal_time_ms}ms\")\n</code></pre>"},{"location":"API_REFERENCE/#message-format","title":"Message Format","text":"<p>All <code>chat()</code> methods expect messages in this format:</p> <pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},  # Optional\n    {\"role\": \"user\", \"content\": \"Hello!\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n    {\"role\": \"user\", \"content\": \"How are you?\"}\n]\n</code></pre> <p>Roles: - <code>\"system\"</code>: System instructions (optional) - <code>\"user\"</code>: User messages - <code>\"assistant\"</code>: Assistant responses</p>"},{"location":"API_REFERENCE/#tool-schema","title":"Tool Schema","text":"<p>Memlayer automatically provides these tools to the LLM:</p>"},{"location":"API_REFERENCE/#search_memory","title":"<code>search_memory</code>","text":"<p>Parameters: - <code>query</code> (string): What to search for - <code>search_tier</code> (string): <code>\"fast\"</code>, <code>\"balanced\"</code>, or <code>\"deep\"</code></p>"},{"location":"API_REFERENCE/#schedule_task","title":"<code>schedule_task</code>","text":"<p>Parameters: - <code>task_description</code> (string): Task description - <code>due_date</code> (string): ISO 8601 date string</p> <p>The LLM calls these tools automatically when needed - no manual configuration required.</p>"},{"location":"API_REFERENCE/#configuration-classes","title":"Configuration Classes","text":""},{"location":"API_REFERENCE/#memlayerconfig","title":"<code>MemlayerConfig</code>","text":"<p>Central configuration object (advanced usage).</p> <pre><code>from memlayer.config import MemlayerConfig\n\nconfig = MemlayerConfig(\n    operation_mode=\"online\",\n    salience_threshold=0.5,\n    embedding_model=\"text-embedding-3-small\"\n)\n</code></pre> <p>Key Attributes: - <code>operation_mode</code>: <code>str</code> - <code>salience_threshold</code>: <code>float</code> - <code>embedding_model</code>: <code>str</code> - <code>chroma_dir</code>: <code>str</code> - <code>networkx_path</code>: <code>str</code> - <code>max_search_results</code>: <code>int</code> - <code>search_tier</code>: <code>str</code></p>"},{"location":"API_REFERENCE/#error-handling","title":"Error Handling","text":"<p>All methods may raise: - <code>ConnectionError</code>: API connection issues - <code>ValueError</code>: Invalid parameters - <code>FileNotFoundError</code>: Storage path issues - Provider-specific exceptions (OpenAI, Anthropic, Google errors)</p> <p>Example:</p> <pre><code>try:\n    response = client.chat(messages)\nexcept ConnectionError as e:\n    print(f\"API connection failed: {e}\")\nexcept ValueError as e:\n    print(f\"Invalid parameters: {e}\")\n</code></pre>"},{"location":"API_REFERENCE/#type-hints","title":"Type Hints","text":"<pre><code>from typing import List, Dict, Generator, Optional, Any\n\n# Message format\nMessage = Dict[str, str]  # {\"role\": str, \"content\": str}\n\n# Chat signatures\ndef chat(\n    messages: List[Message],\n    stream: bool = False,\n    **kwargs: Any\n) -&gt; str | Generator[str, None, None]: ...\n\n# Knowledge format\nKnowledge = Dict[str, List[Dict[str, Any]]]  # {\"facts\": [...], \"entities\": [...], \"relationships\": [...]}\n</code></pre>"},{"location":"API_REFERENCE/#quick-reference","title":"Quick Reference","text":"Method Purpose Returns <code>chat(messages, stream=False)</code> Send chat message <code>str</code> or <code>Generator</code> <code>update_from_text(text)</code> Import knowledge <code>None</code> <code>synthesize_answer(question)</code> Memory-grounded Q&amp;A <code>str</code> or <code>AnswerObject</code> <code>search_service.search(query)</code> Search memory <code>Dict[str, Any]</code> <code>graph_storage.add_task(desc, time)</code> Schedule task <code>str</code> (task_id) <code>analyze_and_extract_knowledge(text)</code> Extract structured data <code>Dict[str, List]</code>"},{"location":"API_REFERENCE/#see-also","title":"See Also","text":"<ul> <li>Overview: Architecture and concepts</li> <li>Quickstart: Getting started guide</li> <li>Streaming: Streaming mode details</li> <li>Examples: Working code examples</li> </ul>"},{"location":"basics/operation_modes/","title":"Memlayer Modes","text":"<p>Memlayer supports three operating modes, each optimized for different use cases.</p> <p>Key Difference: These modes control both salience filtering AND storage architecture.</p>"},{"location":"basics/operation_modes/#local-mode-default","title":"LOCAL Mode (Default)","text":"<p>Best for: High-volume applications, offline usage, no ongoing costs</p> <p>Uses local sentence-transformers models for both salience filtering and vector embeddings.</p> <pre><code>from memlayer.wrappers.openai import OpenAI\n\nclient = OpenAI(\n    storage_path=\"./memories\",\n    user_id=\"user123\",\n    salience_mode=\"local\"  # Default\n)\n</code></pre> <p>Characteristics: - \u2705 High accuracy with semantic understanding - \u2705 No API costs after initial setup - \u2705 Works completely offline - \u2705 Shared model across components (optimized) - \u2705 Full semantic vector search - \u274c Slow startup (~7-8s model loading) - \u274c Requires ~500MB disk space for model</p> <p>Storage: Vector (ChromaDB) + Graph (NetworkX) Startup Time: ~8 seconds (first use) Per-Check Cost: $0 (free) Search Quality: High (semantic similarity)</p>"},{"location":"basics/operation_modes/#online-mode","title":"ONLINE Mode","text":"<p>Best for: Production apps, serverless functions, fast cold starts</p> <p>Uses OpenAI's embeddings API for both salience filtering and vector embeddings.</p> <pre><code>import os\n\nclient = OpenAI(\n    storage_path=\"./memories\",\n    user_id=\"user123\",\n    salience_mode=\"online\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # Required\n)\n</code></pre> <p>Characteristics: - \u2705 Fast startup (~2-3s, no model loading) - \u2705 No local model storage needed - \u2705 Always up-to-date embeddings - \u2705 Scales to serverless/edge environments - \u2705 Full semantic vector search - \u274c API cost per operation (~$0.0001-0.0002) - \u274c Requires internet connection - \u274c Depends on OpenAI API availability</p> <p>Storage: Vector (ChromaDB) + Graph (NetworkX) Startup Time: ~2 seconds Per-Check Cost: ~$0.0001 salience + ~$0.0001 storage (0.02\u00a2 total) Search Quality: High (semantic similarity)</p> <p>Cost Estimate: - 10,000 operations/month = ~$2.00 - 100,000 operations/month = ~$20.00</p>"},{"location":"basics/operation_modes/#lightweight-mode","title":"LIGHTWEIGHT Mode","text":"<p>Best for: Prototyping, resource-constrained environments, maximum speed</p> <p>Uses keyword matching for salience and graph-only storage (no embeddings at all).</p> <pre><code>client = OpenAI(\n    storage_path=\"./memories\",\n    user_id=\"user123\",\n    salience_mode=\"lightweight\"\n)\n</code></pre> <p>Characteristics: - \u2705 Instant startup (&lt; 1s) - \u2705 No dependencies (no ML models) - \u2705 No API costs - \u2705 Minimal memory footprint - \u2705 Perfect for rapid prototyping - \u2705 Graph-based memory retrieval - \u274c No semantic search (keyword/graph only) - \u274c Lower accuracy (rule-based salience) - \u274c May miss nuanced content</p> <p>Storage: Graph-only (NetworkX) - no vector storage Startup Time: &lt; 1 second Per-Check Cost: $0 (free) Search Quality: Medium (graph traversal + keywords)</p>"},{"location":"basics/operation_modes/#comparison-table","title":"Comparison Table","text":"Feature LOCAL ONLINE LIGHTWEIGHT Startup Time ~8s ~2s &lt;1s Per-Operation Cost $0 ~$0.0002 $0 Salience Method Semantic (local) Semantic (API) Keywords Storage Type Vector + Graph Vector + Graph Graph only Search Quality \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Offline Support \u2705 Yes \u274c No \u2705 Yes Disk Space ~500MB ~0MB ~0MB Dependencies sentence-transformers openai None Best For High-volume Production Prototyping"},{"location":"basics/operation_modes/#when-to-use-each-mode","title":"When to Use Each Mode","text":""},{"location":"basics/operation_modes/#use-local-when","title":"Use LOCAL when:","text":"<ul> <li>Running long-lived applications (servers, desktop apps)</li> <li>Processing high volumes (&gt;100k checks/month)</li> <li>Need offline operation</li> <li>Startup time doesn't matter</li> <li>Want zero ongoing costs</li> </ul>"},{"location":"basics/operation_modes/#use-online-when","title":"Use ONLINE when:","text":"<ul> <li>Deploying to serverless (Lambda, Cloud Functions)</li> <li>Need fast cold starts</li> <li>Running on edge/mobile environments</li> <li>Volume is moderate (&lt;100k checks/month)</li> <li>API cost is acceptable</li> </ul>"},{"location":"basics/operation_modes/#use-lightweight-when","title":"Use LIGHTWEIGHT when:","text":"<ul> <li>Rapid prototyping and testing</li> <li>Extremely resource-constrained environments</li> <li>Maximum speed is critical</li> <li>Accuracy requirements are relaxed</li> <li>No internet connectivity</li> </ul>"},{"location":"basics/operation_modes/#benchmarking","title":"Benchmarking","text":"<p>Run the comparison script to see performance on your hardware:</p> <pre><code>python examples/compare_salience_modes.py\n</code></pre> <p>Example output:</p> <pre><code>Mode             Init Time       First Chat      Total First Use\n----------------------------------------------------------------------\nLIGHTWEIGHT        0.234s         2.156s           2.390s\nONLINE             1.892s         2.301s           4.193s\nLOCAL             11.234s         2.189s          13.423s\n</code></pre>"},{"location":"basics/operation_modes/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"basics/operation_modes/#combining-with-custom-thresholds","title":"Combining with Custom Thresholds","text":"<pre><code># Strict LIGHTWEIGHT (only obvious facts)\nclient = OpenAI(\n    salience_mode=\"lightweight\",\n    salience_threshold=0.2  # Higher = stricter\n)\n\n# Permissive ONLINE (save most content)\nclient = OpenAI(\n    salience_mode=\"online\",\n    salience_threshold=-0.05  # Lower = more permissive\n)\n</code></pre>"},{"location":"basics/operation_modes/#mode-specific-tips","title":"Mode-Specific Tips","text":"<p>LOCAL Mode: - Share <code>embedding_model</code> between clients for faster multi-client init - Model caching saves ~11s when creating multiple clients in same process</p> <p>ONLINE Mode: - Prototype embeddings are cached at init time (~2s one-time cost) - Each salience check makes 1 API call (~$0.0001)</p> <p>LIGHTWEIGHT Mode: - Customize keywords by editing <code>SALIENT_KEYWORDS</code> and <code>NON_SALIENT_KEYWORDS</code> in <code>ml_gate.py</code> - Adjust threshold to control sensitivity</p>"},{"location":"basics/operation_modes/#implementation-details","title":"Implementation Details","text":"<p>All three modes share the same two-stage filtering:</p> <ol> <li>Fast Heuristic Filter (&lt; 1ms)</li> <li>Regex pattern matching</li> <li>Catches obvious salient/non-salient content</li> <li> <p>Same across all modes</p> </li> <li> <p>Semantic/Keyword Check (mode-specific)</p> </li> <li>LOCAL: Sentence-transformer embeddings + cosine similarity</li> <li>ONLINE: OpenAI embeddings + cosine similarity  </li> <li>LIGHTWEIGHT: TF-IDF keyword matching</li> </ol>"},{"location":"basics/operation_modes/#migration-guide","title":"Migration Guide","text":""},{"location":"basics/operation_modes/#from-local-to-online","title":"From LOCAL to ONLINE","text":"<pre><code># Before\nclient = OpenAI(salience_mode=\"local\")\n\n# After\nclient = OpenAI(\n    salience_mode=\"online\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n)\n</code></pre> <p>Benefit: 10s faster startup, scales to serverless Cost: ~$0.0001 per salience check</p>"},{"location":"basics/operation_modes/#from-local-to-lightweight","title":"From LOCAL to LIGHTWEIGHT","text":"<pre><code># Before\nclient = OpenAI(salience_mode=\"local\")\n\n# After\nclient = OpenAI(salience_mode=\"lightweight\")\n</code></pre> <p>Benefit: 11s faster startup, no dependencies Trade-off: ~5-10% lower accuracy on edge cases</p>"},{"location":"basics/operation_modes/#faq","title":"FAQ","text":"<p>Q: Can I switch modes after initialization? A: No, mode is set during <code>__init__()</code>. Create a new client to change modes.</p> <p>Q: Which mode is most cost-effective? A: LOCAL for &gt;100k checks/month, ONLINE for &lt;100k, LIGHTWEIGHT for prototyping.</p> <p>Q: Does ONLINE mode require OpenAI API key? A: Yes, it uses OpenAI's embeddings API. Set <code>OPENAI_API_KEY</code> environment variable.</p> <p>Q: Can I use ONLINE mode with other LLM providers? A: Currently only OpenAI embeddings are supported for ONLINE mode. Use LOCAL or LIGHTWEIGHT with other providers.</p> <p>Q: How accurate is LIGHTWEIGHT mode? A: ~80-90% of LOCAL/ONLINE accuracy on typical conversations. Lower on nuanced content.</p>"},{"location":"basics/operation_modes/#next-steps","title":"Next Steps","text":"<ul> <li>Try all three modes with <code>examples/compare_salience_modes.py</code></li> <li>Read the Performance Guide for optimization tips</li> <li>Check Examples for usage patterns</li> </ul>"},{"location":"basics/overview/","title":"Memlayer Overview","text":""},{"location":"basics/overview/#what-is-memlayer","title":"What is Memlayer?","text":"<p>Memlayer is a memory-enhanced LLM wrapper that automatically builds and maintains a persistent knowledge graph from your conversations. It adds memory capabilities to any LLM provider (OpenAI, Claude, Gemini, Ollama) without changing how you interact with them.</p>"},{"location":"basics/overview/#core-architecture","title":"Core Architecture","text":""},{"location":"basics/overview/#how-it-works","title":"How It Works","text":"<ol> <li>Chat Flow: When you send a message via <code>.chat()</code>, Memlayer:</li> <li>Searches the knowledge graph for relevant context</li> <li>Injects that context into the LLM prompt via tool calls</li> <li>Returns the LLM's response to you</li> <li> <p>Asynchronously extracts knowledge and updates the graph</p> </li> <li> <p>Knowledge Extraction: After each conversation turn:</p> </li> <li>Text is analyzed by a fast model (background thread)</li> <li>Facts, entities, and relationships are extracted</li> <li>Salience gate filters out trivial information</li> <li> <p>Knowledge is stored in both vector DB and graph DB</p> </li> <li> <p>Memory Search: When the LLM needs context:</p> </li> <li>Hybrid search combines vector similarity + graph traversal</li> <li>Three search tiers available: <code>fast</code>, <code>balanced</code>, <code>deep</code></li> <li> <p>Results are ranked and returned as context</p> </li> <li> <p>Background Services:</p> </li> <li>Consolidation: Extracts knowledge from conversations (async)</li> <li>Curation: Expires time-sensitive facts (background thread)</li> <li>Salience Gate: Filters low-value information before storage</li> </ol>"},{"location":"basics/overview/#data-flow","title":"Data Flow","text":""},{"location":"basics/overview/#normal-chat-non-streaming","title":"Normal Chat (Non-Streaming)","text":"<pre><code>User Message\n    \u2502\n    \u25bc\nMemory Search (if LLM calls tool)\n    \u2502\n    \u25bc\nLLM Response Generated\n    \u2502\n    \u251c\u2500\u25ba Return to User\n    \u2502\n    \u2514\u2500\u25ba Background: Extract Knowledge \u2192 Store in Graph\n</code></pre>"},{"location":"basics/overview/#streaming-chat","title":"Streaming Chat","text":"<pre><code>User Message\n    \u2502\n    \u25bc\nMemory Search (if LLM calls tool)\n    \u2502\n    \u25bc\nLLM Starts Streaming\n    \u2502\n    \u251c\u2500\u25ba Yield chunks to user in real-time\n    \u2502\n    \u2514\u2500\u25ba Background: Buffer full response\n            \u2502\n            \u2514\u2500\u25ba After stream completes \u2192 Extract Knowledge \u2192 Store in Graph\n</code></pre>"},{"location":"basics/overview/#key-features","title":"Key Features","text":""},{"location":"basics/overview/#1-provider-agnostic","title":"1. Provider-Agnostic","text":"<p>Works with OpenAI, Anthropic Claude, Google Gemini, and local Ollama models. Same API across all providers.</p>"},{"location":"basics/overview/#2-automatic-memory-tools","title":"2. Automatic Memory Tools","text":"<p>LLM automatically gets access to: - <code>search_memory</code>: Hybrid vector + graph search - <code>schedule_task</code>: Create time-based reminders</p>"},{"location":"basics/overview/#3-flexible-search-tiers","title":"3. Flexible Search Tiers","text":"<ul> <li>fast: Vector-only search, &lt;100ms</li> <li>balanced: Vector + 1-hop graph traversal</li> <li>deep: Full graph traversal with entity extraction</li> </ul>"},{"location":"basics/overview/#4-knowledge-graph-features","title":"4. Knowledge Graph Features","text":"<ul> <li>Entity deduplication (e.g., \"John\" = \"John Smith\")</li> <li>Relationship tracking between entities</li> <li>Time-aware facts with expiration dates</li> <li>Importance scoring for fact prioritization</li> </ul>"},{"location":"basics/overview/#5-operation-modes","title":"5. Operation Modes","text":"<p>Choose embedding strategy based on your needs: - online: API-based embeddings (OpenAI), fast startup - local: Local sentence-transformer model, no API costs - lightweight: Graph-only, no embeddings, fastest startup</p>"},{"location":"basics/overview/#configuration-options","title":"Configuration Options","text":"<pre><code>from memlayer.wrappers.openai import OpenAI\n\nclient = OpenAI(\n    # Core settings\n    api_key=\"your-key\",\n    model=\"gpt-4.1-mini\",\n    user_id=\"user123\",\n\n    # Memory behavior\n    operation_mode=\"online\",        # online | local | lightweight\n    salience_threshold=0.5,         # 0.0-1.0, filters trivial content\n\n    # Storage paths\n    chroma_dir=\"./my_chroma_db\",\n    networkx_path=\"./my_graph.pkl\",\n\n    # Search behavior\n    max_search_results=5,\n    search_tier=\"balanced\",          # fast | balanced | deep\n\n    # Performance tuning\n    curation_interval=3600,          # Check for expired facts every hour\n    embedding_model=\"text-embedding-3-small\"\n)\n</code></pre>"},{"location":"basics/overview/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"basics/overview/#basic-chat","title":"Basic Chat","text":"<pre><code>response = client.chat([\n    {\"role\": \"user\", \"content\": \"My name is Alice\"}\n])\n# Knowledge automatically extracted and stored\n</code></pre>"},{"location":"basics/overview/#streaming-chat_1","title":"Streaming Chat","text":"<pre><code>for chunk in client.chat(\n    [{\"role\": \"user\", \"content\": \"What's my name?\"}],\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"basics/overview/#direct-knowledge-ingestion","title":"Direct Knowledge Ingestion","text":"<pre><code># Import knowledge from documents\nclient.update_from_text(\"\"\"\nProject Phoenix is led by Alice.\nThe project deadline is December 1st.\n\"\"\")\n</code></pre>"},{"location":"basics/overview/#synthesized-qa","title":"Synthesized Q&amp;A","text":"<pre><code># Get memory-grounded answer\nanswer = client.synthesize_answer(\"Who leads Project Phoenix?\")\n</code></pre>"},{"location":"basics/overview/#performance-characteristics","title":"Performance Characteristics","text":"Component Latency Notes Memory search (fast) 50-100ms Vector search only Memory search (balanced) 100-300ms Vector + 1-hop graph Memory search (deep) 300-1000ms Full graph traversal Knowledge extraction 1-3s Background, doesn't block response Consolidation 1-2s Async, uses fast model First-time salience init 1-2s Cached after first run"},{"location":"basics/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the right operation mode:</li> <li>Serverless \u2192 <code>online</code> mode</li> <li>Privacy-sensitive \u2192 <code>local</code> mode</li> <li> <p>Demos/prototypes \u2192 <code>lightweight</code> mode</p> </li> <li> <p>Use streaming for better UX:</p> </li> <li>First chunk arrives in 1-3s</li> <li>Knowledge extraction happens in background</li> <li> <p>User sees response immediately</p> </li> <li> <p>Tune salience threshold:</p> </li> <li>Low (0.3-0.5): Keep more memories, higher storage</li> <li>Medium (0.5-0.7): Balanced, recommended default</li> <li> <p>High (0.7-0.9): Only important facts, minimal storage</p> </li> <li> <p>Set expiration dates for time-sensitive facts:</p> </li> <li>System automatically extracts expiration dates from text</li> <li> <p>Curation service removes expired facts periodically</p> </li> <li> <p>Use appropriate search tier:</p> </li> <li><code>fast</code>: Quick lookups, high-traffic applications</li> <li><code>balanced</code>: Default, good recall with reasonable latency</li> <li><code>deep</code>: Complex questions needing graph reasoning</li> </ol>"},{"location":"basics/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart Guide: Get up and running in 5 minutes</li> <li>Streaming Mode: Deep dive into streaming behavior</li> <li>Operation Modes: Architecture implications of each mode</li> <li>Provider Setup: Provider-specific configuration</li> </ul>"},{"location":"basics/quickstart/","title":"Memlayer Quickstart","text":"<p>Get started with Memlayer in under 5 minutes. This guide shows you how to add persistent memory to any LLM.</p>"},{"location":"basics/quickstart/#installation","title":"Installation","text":"<pre><code>pip install memlayer\n</code></pre>"},{"location":"basics/quickstart/#provider-specific-dependencies","title":"Provider-Specific Dependencies","text":"<p>Install the SDK for your chosen provider:</p> <pre><code># OpenAI\npip install openai\n\n# Anthropic Claude\npip install anthropic\n\n# Google Gemini\npip install google-generativeai\n\n# Ollama (local models)\npip install ollama\n</code></pre>"},{"location":"basics/quickstart/#quick-start-examples","title":"Quick Start Examples","text":""},{"location":"basics/quickstart/#openai","title":"OpenAI","text":"<pre><code>from memlayer.wrappers.openai import OpenAI\n\n# Initialize with memory\nclient = OpenAI(\n    api_key=\"your-openai-api-key\",\n    model=\"gpt-4.1-mini\",\n    user_id=\"alice\"\n)\n\n# First conversation - teach it something\nresponse = client.chat([\n    {\"role\": \"user\", \"content\": \"My name is Alice and I work on Project Phoenix\"}\n])\nprint(response)\n\n# Later conversation - it remembers!\nresponse = client.chat([\n    {\"role\": \"user\", \"content\": \"What project do I work on?\"}\n])\nprint(response)\n# Output: \"You work on Project Phoenix.\"\n</code></pre>"},{"location":"basics/quickstart/#anthropic-claude","title":"Anthropic Claude","text":"<pre><code>from memlayer.wrappers.claude import Claude\n\nclient = Claude(\n    api_key=\"your-anthropic-api-key\",\n    model=\"claude-3-5-sonnet-20241022\",\n    user_id=\"alice\"\n)\n\n# Use exactly like OpenAI wrapper\nresponse = client.chat([\n    {\"role\": \"user\", \"content\": \"Remember: my favorite color is blue\"}\n])\n</code></pre>"},{"location":"basics/quickstart/#google-gemini","title":"Google Gemini","text":"<pre><code>from memlayer.wrappers.gemini import Gemini\n\nclient = Gemini(\n    api_key=\"your-gemini-api-key\",\n    model=\"gemini-2.5-flash\",\n    user_id=\"alice\"\n)\n\nresponse = client.chat([\n    {\"role\": \"user\", \"content\": \"I live in San Francisco\"}\n])\n</code></pre>"},{"location":"basics/quickstart/#ollama-local-models","title":"Ollama (Local Models)","text":"<pre><code>from memlayer.wrappers.ollama import Ollama\n\n# Make sure Ollama server is running: ollama serve\nclient = Ollama(\n    model=\"llama3.2\",\n    host=\"http://localhost:11434\",\n    user_id=\"alice\",\n    operation_mode=\"local\"  # Use local embeddings too\n)\n\nresponse = client.chat([\n    {\"role\": \"user\", \"content\": \"My dog's name is Max\"}\n])\n</code></pre>"},{"location":"basics/quickstart/#environment-variables","title":"Environment Variables","text":"<p>Instead of passing API keys in code, use environment variables:</p> <pre><code># OpenAI\nexport OPENAI_API_KEY=\"your-key\"\n\n# Anthropic Claude\nexport ANTHROPIC_API_KEY=\"your-key\"\n\n# Google Gemini\nexport GOOGLE_API_KEY=\"your-key\"\n</code></pre> <p>Then initialize without the <code>api_key</code> parameter:</p> <pre><code>from memlayer.wrappers.openai import OpenAI\n\nclient = OpenAI(\n    model=\"gpt-4.1-mini\",\n    user_id=\"alice\"\n)\n</code></pre>"},{"location":"basics/quickstart/#basic-usage-patterns","title":"Basic Usage Patterns","text":""},{"location":"basics/quickstart/#1-regular-chat-non-streaming","title":"1. Regular Chat (Non-Streaming)","text":"<pre><code># Single turn\nresponse = client.chat([\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n])\n\n# Multi-turn conversation\nmessages = [\n    {\"role\": \"user\", \"content\": \"My birthday is May 15th\"},\n    {\"role\": \"assistant\", \"content\": \"I'll remember that!\"},\n    {\"role\": \"user\", \"content\": \"When is my birthday?\"}\n]\nresponse = client.chat(messages)\n</code></pre>"},{"location":"basics/quickstart/#2-streaming-chat","title":"2. Streaming Chat","text":"<pre><code># Stream response chunks as they arrive\nfor chunk in client.chat(\n    [{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\nprint()  # Newline after stream completes\n</code></pre>"},{"location":"basics/quickstart/#3-direct-knowledge-import","title":"3. Direct Knowledge Import","text":"<pre><code># Import knowledge from documents/emails/notes\nclient.update_from_text(\"\"\"\nMeeting Notes - Nov 15, 2025:\n- Q4 deadline is December 20th\n- Budget increased by 15%\n- New team member: Bob (joins Monday)\n\"\"\")\n\n# Now the LLM can answer questions about this\nresponse = client.chat([\n    {\"role\": \"user\", \"content\": \"When is the Q4 deadline?\"}\n])\n# Output: \"The Q4 deadline is December 20th.\"\n</code></pre>"},{"location":"basics/quickstart/#4-memory-grounded-qa","title":"4. Memory-Grounded Q&amp;A","text":"<pre><code># Get a synthesized answer with sources\nanswer_obj = client.synthesize_answer(\n    \"What do we know about Project Phoenix?\",\n    return_object=True\n)\n\nprint(f\"Answer: {answer_obj.answer}\")\nprint(f\"Sources: {answer_obj.sources}\")\nprint(f\"Confidence: {answer_obj.confidence}\")\n</code></pre>"},{"location":"basics/quickstart/#configuration-basics","title":"Configuration Basics","text":""},{"location":"basics/quickstart/#user-isolation","title":"User Isolation","text":"<p>Each <code>user_id</code> gets an isolated memory space:</p> <pre><code>alice_client = OpenAI(model=\"gpt-4.1-mini\", user_id=\"alice\")\nbob_client = OpenAI(model=\"gpt-4.1-mini\", user_id=\"bob\")\n\n# Alice's memories don't leak to Bob\nalice_client.chat([{\"role\": \"user\", \"content\": \"My secret is XYZ\"}])\nbob_response = bob_client.chat([{\"role\": \"user\", \"content\": \"What's Alice's secret?\"}])\n# Bob won't know - different memory spaces\n</code></pre>"},{"location":"basics/quickstart/#storage-paths","title":"Storage Paths","text":"<p>Customize where memories are stored:</p> <pre><code>client = OpenAI(\n    model=\"gpt-4.1-mini\",\n    user_id=\"alice\",\n    chroma_dir=\"./memories/vector_db\",      # Vector embeddings\n    networkx_path=\"./memories/graph.pkl\"    # Knowledge graph\n)\n</code></pre>"},{"location":"basics/quickstart/#operation-modes","title":"Operation Modes","text":"<p>Choose how embeddings are computed:</p> <pre><code># Online mode (default) - uses OpenAI API for embeddings\nclient = OpenAI(model=\"gpt-4.1-mini\", operation_mode=\"online\")\n\n# Local mode - uses local sentence-transformer (no API calls)\nclient = OpenAI(model=\"gpt-4.1-mini\", operation_mode=\"local\")\n\n# Lightweight mode - no embeddings, graph-only (fastest startup)\nclient = OpenAI(model=\"gpt-4.1-mini\", operation_mode=\"lightweight\")\n</code></pre>"},{"location":"basics/quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"basics/quickstart/#persistent-sessions","title":"Persistent Sessions","text":"<pre><code># Initialize once, reuse across application lifetime\nclient = OpenAI(model=\"gpt-4.1-mini\", user_id=\"alice\")\n\n# All conversations automatically build on previous memories\nclient.chat([{\"role\": \"user\", \"content\": \"I like pizza\"}])\n# ... later ...\nclient.chat([{\"role\": \"user\", \"content\": \"What food do I like?\"}])\n# Remembers: \"You like pizza\"\n</code></pre>"},{"location":"basics/quickstart/#conversation-history-management","title":"Conversation History Management","text":"<pre><code># Memlayer handles memory automatically, but you control conversation history\nconversation = []\n\n# Turn 1\nconversation.append({\"role\": \"user\", \"content\": \"My name is Alice\"})\nresponse = client.chat(conversation)\nconversation.append({\"role\": \"assistant\", \"content\": response})\n\n# Turn 2\nconversation.append({\"role\": \"user\", \"content\": \"What's my name?\"})\nresponse = client.chat(conversation)\n# LLM can answer from:\n# 1. Conversation history (conversation list)\n# 2. Long-term memory (knowledge graph)\n</code></pre>"},{"location":"basics/quickstart/#time-sensitive-facts","title":"Time-Sensitive Facts","text":"<pre><code># System automatically extracts expiration dates\nclient.chat([{\n    \"role\": \"user\", \n    \"content\": \"The temporary password is 1234, valid for 24 hours\"\n}])\n\n# After 24 hours, this fact is automatically removed by curation service\n</code></pre>"},{"location":"basics/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Streaming Mode Guide: Learn about streaming responses</li> <li>Operation Modes: Architecture implications</li> <li>Search Tiers: Optimize search performance</li> <li>Ollama Setup: Run completely offline with local models</li> <li>Examples: Browse complete working examples</li> </ul>"},{"location":"basics/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"basics/quickstart/#no-module-named-memlayer","title":"\"No module named 'memlayer'\"","text":"<pre><code>pip install memlayer\n</code></pre>"},{"location":"basics/quickstart/#api-key-not-found","title":"\"API key not found\"","text":"<p>Set your environment variable or pass <code>api_key</code> parameter:</p> <pre><code>client = OpenAI(api_key=\"your-key\", ...)\n</code></pre>"},{"location":"basics/quickstart/#ollama-connection-refused","title":"\"Ollama connection refused\"","text":"<p>Start the Ollama server:</p> <pre><code>ollama serve\n</code></pre>"},{"location":"basics/quickstart/#slow-first-response","title":"Slow first response","text":"<p>First call initializes salience gate (~1-2s). Subsequent calls are fast. Use <code>operation_mode=\"lightweight\"</code> for instant startup in demos.</p>"},{"location":"basics/quickstart/#memory-not-persisting","title":"Memory not persisting","text":"<p>Check that <code>chroma_dir</code> and <code>networkx_path</code> are writable directories. By default, they're created in the current working directory.</p>"},{"location":"basics/streaming/","title":"Streaming Mode","text":"<p>Memlayer supports streaming responses from all providers (OpenAI, Claude, Gemini, Ollama). This guide explains how streaming works, its performance characteristics, and best practices.</p>"},{"location":"basics/streaming/#what-is-streaming","title":"What is Streaming?","text":"<p>Streaming mode yields response chunks as they're generated by the LLM, rather than waiting for the complete response. This provides a better user experience with faster perceived latency.</p>"},{"location":"basics/streaming/#basic-usage","title":"Basic Usage","text":""},{"location":"basics/streaming/#streaming-with-openai","title":"Streaming with OpenAI","text":"<pre><code>from memlayer.wrappers.openai import OpenAI\n\nclient = OpenAI(model=\"gpt-4.1-mini\", user_id=\"alice\")\n\n# Enable streaming with stream=True\nfor chunk in client.chat(\n    [{\"role\": \"user\", \"content\": \"Tell me about machine learning\"}],\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\nprint()  # Newline after completion\n</code></pre>"},{"location":"basics/streaming/#streaming-with-claude","title":"Streaming with Claude","text":"<pre><code>from memlayer.wrappers.claude import Claude\n\nclient = Claude(model=\"claude-3-5-sonnet-20241022\", user_id=\"alice\")\n\nfor chunk in client.chat(\n    [{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"basics/streaming/#streaming-with-gemini","title":"Streaming with Gemini","text":"<pre><code>from memlayer.wrappers.gemini import Gemini\n\nclient = Gemini(model=\"gemini-2.5-flash\", user_id=\"alice\")\n\nfor chunk in client.chat(\n    [{\"role\": \"user\", \"content\": \"What is a neural network?\"}],\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"basics/streaming/#streaming-with-ollama","title":"Streaming with Ollama","text":"<pre><code>from memlayer.wrappers.ollama import Ollama\n\nclient = Ollama(model=\"llama3.2\", user_id=\"alice\")\n\nfor chunk in client.chat(\n    [{\"role\": \"user\", \"content\": \"Describe photosynthesis\"}],\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"basics/streaming/#memory-search-with-streaming","title":"Memory Search with Streaming","text":"<p>When using memory search, the LLM may call the <code>search_memory</code> tool before streaming:</p> <pre><code># Message that triggers memory search\nfor chunk in client.chat([\n    {\"role\": \"user\", \"content\": \"What did I tell you about my project?\"}\n], stream=True):\n    print(chunk, end=\"\", flush=True)\n\n# Timeline:\n# t=0ms:    Message sent\n# t=50ms:   LLM calls search_memory tool\n# t=250ms:  Search completes, context retrieved\n# t=1500ms: First chunk arrives (including search time)\n# t=1500ms+: Chunks stream in real-time\n</code></pre>"},{"location":"basics/streaming/#knowledge-extraction-background","title":"Knowledge Extraction (Background)","text":"<p>After streaming completes, Memlayer extracts knowledge in a background thread:</p> <pre><code>for chunk in client.chat([\n    {\"role\": \"user\", \"content\": \"I work at Acme Corp\"}\n], stream=True):\n    print(chunk, end=\"\", flush=True)\n\n# Your code continues immediately after streaming\nprint(\"Streaming done!\")  # This prints right away\n\n# Meanwhile, in background:\n# - Salience check (~100ms, cached after first run)\n# - Knowledge extraction API call (1-2s, fast model)\n# - Graph update (~50ms)\n</code></pre>"},{"location":"basics/streaming/#next-steps","title":"Next Steps","text":"<ul> <li>Overview: Understand the full architecture</li> <li>Quickstart: Get started in 5 minutes</li> <li>Operation Modes: Choose the right mode</li> <li>Examples: See complete working code</li> </ul>"},{"location":"providers/","title":"Provider-Specific Documentation","text":"<p>Memlayer supports multiple LLM providers with a unified API. Each provider has specific configuration requirements and features documented here.</p>"},{"location":"providers/#supported-providers","title":"Supported Providers","text":""},{"location":"providers/#openai","title":"OpenAI","text":"<ul> <li>Models: GPT-4.1, GPT 5 etc</li> <li>Streaming: \u2705 Full support</li> <li>Best for: Production applications, fastest API responses</li> <li>Setup: Requires <code>OPENAI_API_KEY</code> environment variable</li> </ul>"},{"location":"providers/#anthropic-claude","title":"Anthropic Claude","text":"<ul> <li>Models: Claude 4.5 Sonnet, Claude 4 Opus, Claude 4 Haiku</li> <li>Streaming: \u2705 Full support</li> <li>Best for: Long conversations, complex reasoning</li> <li>Setup: Requires <code>ANTHROPIC_API_KEY</code> environment variable</li> </ul>"},{"location":"providers/#google-gemini","title":"Google Gemini","text":"<ul> <li>Models: Gemini 2.5 Flash, Gemini 2.5 Pro</li> <li>Streaming: \u2705 Full support</li> <li>Best for: Multimodal applications, cost efficiency</li> <li>Setup: Requires <code>GOOGLE_API_KEY</code> environment variable</li> </ul>"},{"location":"providers/#ollama-local-models","title":"Ollama (Local Models)","text":"<ul> <li>Models: Llama 3.2, Llama 3.1, Mistral, Phi 3, 100+ more</li> <li>Streaming: \u2705 Full support</li> <li>Best for: Privacy, offline use, zero API costs</li> <li>Setup: Requires local Ollama server (<code>ollama serve</code>)</li> </ul>"},{"location":"providers/#lmstudio-local-models","title":"LMStudio (Local Models)","text":"<ul> <li>Models: Llama 4, Qwen 3, 100+ more</li> <li>Streaming: \u2705 Full support</li> <li>Best for: Privacy, offline use, zero API costs</li> <li>Setup: Requires local LMStudio server</li> </ul>"},{"location":"providers/#quick-comparison","title":"Quick Comparison","text":"Provider API Cost Latency Privacy Offline OpenAI $$ Fast Cloud \u274c Claude $$ Fast Cloud \u274c Gemini $ Fast Cloud \u274c Ollama Free Medium Local \u2705 LMStudio Free Medium Local \u2705"},{"location":"providers/#configuration-basics","title":"Configuration Basics","text":"<p>All providers share the same Memlayer API:</p> <pre><code>from memlayer import OpenAI\nfrom memlayer import Claude\nfrom memlayer import Gemini\nfrom memlayer import Ollama\nfrom memlayer import LMStudio\n# OpenAI\nclient = OpenAI(\n    api_key=\"your-key\",\n    model=\"gpt-4.1-mini\",\n    user_id=\"alice\"\n)\n\n# Claude\nclient = Claude(\n    api_key=\"your-key\",\n    model=\"claude-3-5-sonnet-20241022\",\n    user_id=\"alice\"\n)\n\n# Gemini\nclient = Gemini(\n    api_key=\"your-key\",\n    model=\"gemini-2.5-flash\",\n    user_id=\"alice\"\n)\n\n# Ollama (local)\nclient = Ollama(\n    model=\"llama3.2\",\n    host=\"http://localhost:11434\",\n    user_id=\"alice\",\n    operation_mode=\"local\"  # Fully offline\n)\n\nclient = LMStudio(\n    model=\"llama3.2\",\n    host=\"http://localhost:1234/v1\",\n    user_id=\"alice\",\n    operation_mode=\"local\"  # Fully offline\n)\n</code></pre>"},{"location":"providers/#common-features-across-all-providers","title":"Common Features Across All Providers","text":""},{"location":"providers/#memory-knowledge-graph","title":"Memory &amp; Knowledge Graph","text":"<p>All providers support: - \u2705 Automatic knowledge extraction - \u2705 Persistent memory across sessions - \u2705 Hybrid search (vector + graph) - \u2705 Time-aware facts with expiration - \u2705 User-isolated memory spaces</p>"},{"location":"providers/#streaming-responses","title":"Streaming Responses","text":"<p>All providers support streaming:</p> <pre><code>for chunk in client.chat([\n    {\"role\": \"user\", \"content\": \"Tell me a story\"}\n], stream=True):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"providers/#operation-modes","title":"Operation Modes","text":"<p>All providers support three operation modes: - online: API-based embeddings (fast startup) - local: Local embeddings (privacy, offline) - lightweight: No embeddings (instant startup)</p>"},{"location":"providers/#provider-specific-pages","title":"Provider-Specific Pages","text":"<p>Click on any provider below for detailed setup instructions:</p> <ul> <li>openai.md \u2014 OpenAI configuration, models, and tips</li> <li>claude.md \u2014 Anthropic Claude setup and features</li> <li>gemini.md \u2014 Google Gemini configuration</li> <li>ollama.md \u2014 \ud83c\udd95 Complete guide to local models: installation, model recommendations, fully offline setup</li> <li>lmstudio.md \u2014 \ud83c\udd95 Complete guide to LMStudio local models: installation, model recommendations, fully offline setup</li> </ul>"},{"location":"providers/#getting-started","title":"Getting Started","text":"<ol> <li>Choose a provider based on your needs (cost, privacy, performance)</li> <li>Set up credentials (see individual provider pages)</li> <li>Follow the quickstart \u2014 docs/basics/quickstart.md</li> <li>Enable streaming (optional) \u2014 docs/basics/streaming.md</li> </ol>"},{"location":"providers/#related-documentation","title":"Related Documentation","text":"<ul> <li>Basics Overview: How Memlayer works</li> <li>Quickstart Guide: Get started in 5 minutes</li> <li>Streaming Mode: Stream responses from any provider</li> <li>Operation Modes: Choose online, local, or lightweight mode</li> <li>Examples: Working code for each provider</li> </ul>"},{"location":"providers/claude/","title":"Claude (Anthropic) Provider Notes","text":""},{"location":"providers/claude/#credentials","title":"Credentials","text":"<ul> <li>Set <code>ANTHROPIC_API_KEY</code> in your environment or pass <code>api_key</code> to <code>Claude(...)</code>.</li> <li>If you don't have a key, the client can still be used for local-only features (consolidation via other provider clients).</li> </ul>"},{"location":"providers/claude/#notes","title":"Notes","text":"<ul> <li>Claude's SDK requires <code>api_key</code> or <code>auth_token</code>. If neither is set, calls to <code>client.messages.create</code> will fail.</li> <li>You can use OpenAI embeddings in <code>operation_mode=online</code> even when using Claude as the chat provider.</li> </ul> <p>\u00bb See also: <code>docs/tuning/operation_mode.md</code></p>"},{"location":"providers/gemini/","title":"Gemini (Google) Provider Notes","text":""},{"location":"providers/gemini/#credentials","title":"Credentials","text":"<ul> <li>Set <code>GOOGLE_API_KEY</code> or use ADC (Application Default Credentials) depending on your environment.</li> </ul>"},{"location":"providers/gemini/#notes","title":"Notes","text":"<ul> <li>Gemini client in this repo uses <code>google-genai</code> types and requires a working genai client in your environment.</li> <li>Check <code>examples/05_providers/gemini_example.py</code> for an end-to-end sample.</li> </ul> <p>\u00bb See also: <code>docs/tuning/operation_mode.md</code></p>"},{"location":"providers/lmstudio/","title":"LM Studio: Local LLM Provider","text":""},{"location":"providers/lmstudio/#overview","title":"Overview","text":"<p>LM Studio is a user-friendly desktop application that allows you to discover, download, and run local LLMs with a graphical interface. It provides a local server that mimics the OpenAI API, making it incredibly easy to integrate with Memlayer.</p> <p>Key Benefits: - \u2705 GUI Interface: Easy to find and test models (GGUF format) - \u2705 Hardware Optimization: Easy GPU offloading sliders (NVIDIA, AMD, Apple Silicon) - \u2705 Privacy: Completely offline operation - \u2705 Compatibility: Drop-in replacement for OpenAI-based workflows</p>"},{"location":"providers/lmstudio/#installation","title":"Installation","text":""},{"location":"providers/lmstudio/#1-install-lm-studio","title":"1. Install LM Studio","text":"<p>Download and install the application for your OS (macOS, Windows, or Linux) from lmstudio.ai.</p>"},{"location":"providers/lmstudio/#2-install-memlayer","title":"2. Install Memlayer","text":"<pre><code>pip install memlayer openai\n</code></pre> <p>(Note: The <code>openai</code> library is required because Memlayer communicates with LM Studio using the OpenAI protocol).</p>"},{"location":"providers/lmstudio/#quick-start","title":"Quick Start","text":""},{"location":"providers/lmstudio/#1-prepare-lm-studio","title":"1. Prepare LM Studio","text":"<ol> <li>Open LM Studio.</li> <li>Click the Magnifying Glass (Search) icon.</li> <li>Search for a model (e.g., <code>qwen 2.5 7b</code> or <code>llama 3.1 8b</code>).</li> <li>Download a quantization level that fits your RAM (e.g., <code>Q4_K_M</code> or <code>Q6_K</code>).</li> </ol>"},{"location":"providers/lmstudio/#2-start-the-local-server","title":"2. Start the Local Server","text":"<ol> <li>Click the Developer / Local Server icon (usually <code>&lt; &gt;</code> on the left sidebar).</li> <li>Select your downloaded model from the top dropdown to load it into memory.</li> <li>Click the green Start Server button.</li> <li>Note the URL (Default: <code>http://localhost:1234</code>).</li> </ol>"},{"location":"providers/lmstudio/#3-basic-usage","title":"3. Basic Usage","text":"<pre><code>from memlayer.wrappers.lmstudio import LMStudio\n\n# Initialize client pointing to local server\nclient = LMStudio(\n    base_url=\"http://localhost:1234/v1\",\n    api_key=\"lm-studio\",    # Any string works\n    user_id=\"alice\",\n    operation_mode=\"local\"  # Important: Keeps embeddings local too\n)\n\n# Chat\nresponse = client.chat([\n    {\"role\": \"user\", \"content\": \"My name is Alice and I'm a sci-fi writer.\"}\n])\nprint(response)\n\n# Memory Recall\nresponse = client.chat([\n    {\"role\": \"user\", \"content\": \"What is my profession?\"}\n])\nprint(response)  # \"You are a sci-fi writer.\"\n</code></pre>"},{"location":"providers/lmstudio/#recommended-models","title":"Recommended Models","text":"<p>Since Memlayer relies on Tool Calling and JSON Extraction for memory management, you must use models capable of instruction following.</p>"},{"location":"providers/lmstudio/#for-speed-2s-response","title":"For Speed (&lt; 2s response)","text":"<ul> <li>Gemma 3 (1B\u20133B, Instruct) \u2013 Extremely fast, long context, very efficient.</li> <li>Instella-3B (Instruct) \u2013 New 2025 lightweight model optimized for instruction-following.</li> <li>Mistral Small 3.1 (Efficient 24B, Instruct) \u2013 Higher params but highly optimized for low-latency inference.</li> </ul>"},{"location":"providers/lmstudio/#for-quality-standard","title":"For Quality (Standard)","text":"<ul> <li>Qwen 3 (32B, Instruct) \u2013 Excellent logic, tool use, and long context.</li> <li>Llama 4 (8B\u201370B, Scout/Maverick variants) \u2013 The new industry standard for local models in 2025.</li> <li>Mistral Medium 3 (~24\u201332B, Instruct) \u2013 Strong balance of performance and compute cost.</li> </ul>"},{"location":"providers/lmstudio/#for-best-performance","title":"For Best Performance","text":"<ul> <li>Qwen 3 (235B-A22B hybrid) \u2013 State-of-the-art reasoning with massive context windows.</li> <li>Llama 4 Behemoth (Large-scale) \u2013 High-end open model with near GPT-4.5-class capability.</li> </ul>"},{"location":"providers/lmstudio/#configuration","title":"Configuration","text":""},{"location":"providers/lmstudio/#complete-configuration-example","title":"Complete Configuration Example","text":"<pre><code>from memlayer import LMStudio\n\nclient = LMStudio(\n    # Server Connection\n    base_url=\"http://localhost:1234/v1\",\n    api_key=\"lm-studio\",\n    model=\"local-model\", # Usually ignored by LM Studio, but good for logging\n\n    # Memory &amp; Privacy\n    user_id=\"alice\",\n    operation_mode=\"local\", # \"local\", \"online\", or \"lightweight\"\n\n    # Storage Paths\n    storage_path=\"./memlayer_data\",\n\n    # Tuning\n    temperature=0.7,\n)\n</code></pre>"},{"location":"providers/lmstudio/#operation-modes","title":"Operation Modes","text":"<p>Local Mode (Recommended for LM Studio users):</p> <pre><code>client = LMStudio(operation_mode=\"local\")\n</code></pre> <ul> <li>LLM: Local (LM Studio)</li> <li>Embeddings: Local (HuggingFace <code>all-MiniLM-L6-v2</code> running in Python)</li> <li>Privacy: 100% Offline.</li> </ul> <p>Online Mode:</p> <pre><code>import os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nclient = LMStudio(operation_mode=\"online\")\n</code></pre> <ul> <li>LLM: Local (LM Studio)</li> <li>Embeddings: OpenAI API (<code>text-embedding-3-small</code>)</li> <li>Privacy: Text leaves machine for embedding, but inference is local.</li> </ul>"},{"location":"providers/lmstudio/#streaming-support","title":"Streaming Support","text":"<p>Memlayer supports streaming with LM Studio just like any other provider.</p> <pre><code>from memlayer import LMStudio\n\nclient = LMStudio(operation_mode=\"local\")\n\nprint(\"Assistant: \", end=\"\")\nfor chunk in client.chat([\n    {\"role\": \"user\", \"content\": \"Write a short poem about memory.\"}\n], stream=True):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"providers/lmstudio/#performance-tuning","title":"Performance Tuning","text":""},{"location":"providers/lmstudio/#1-gpu-offloading","title":"1. GPU Offloading","text":"<p>In the LM Studio Local Server tab (right sidebar), look for \"GPU Offload\". *   Max: Slide to the right to put as many layers as possible on your GPU. *   Impact: Drastically improves speed.</p>"},{"location":"providers/lmstudio/#2-context-window","title":"2. Context Window","text":"<p>In the LM Studio sidebar, check Context Length. *   Ensure it is at least <code>2048</code> or <code>4096</code>. *   If Memlayer memories grow large, you may need <code>8192</code> (if your model supports it).</p>"},{"location":"providers/lmstudio/#3-system-prompt","title":"3. System Prompt","text":"<p>In LM Studio settings: *   System Prompt: Ensure this is enabled. *   Memlayer injects instructions about \"Using Tools\" into the system prompt. If the model behaves unexpectedly, ensure LM Studio isn't overriding the system prompt sent by the client.</p>"},{"location":"providers/lmstudio/#troubleshooting","title":"Troubleshooting","text":""},{"location":"providers/lmstudio/#connection-refused-or-target-machine-actively-refused","title":"\"Connection Refused\" or \"Target Machine Actively Refused\"","text":"<p>Cause: The LM Studio server is not running. Solution: Go to the \"Local Server\" tab in LM Studio and click the green Start Server button.</p>"},{"location":"providers/lmstudio/#error-400-response_format","title":"\"Error: 400... response_format\"","text":"<p>Cause: Older versions of LM Studio or specific model loaders may not support the <code>json_object</code> enforcement used by OpenAI. Solution: Memlayer's <code>LMStudio</code> wrapper handles this automatically by stripping strict JSON flags. Ensure you are using the latest version of Memlayer.</p>"},{"location":"providers/lmstudio/#complete-example","title":"Complete Example","text":"<pre><code>from memlayer import LMStudio\nimport time\n\n# 1. Setup Client\nclient = LMStudio(\n    base_url=\"http://localhost:1234/v1\",\n    model=\"qwen3-14b\",\n    user_id=\"alice_local\"\n)\n\ndef chat_with_memory(text):\n    print(f\"\\n\ud83d\udc64 User: {text}\")\n    print(f\"\ud83e\udd16 AI: \", end=\"\", flush=True)\n\n    # Stream response\n    full_response = \"\"\n    for chunk in client.chat([{\"role\": \"user\", \"content\": text}], stream=True):\n        print(chunk, end=\"\", flush=True)\n        full_response += chunk\n    print()\n\n# 2. Teach\nchat_with_memory(\"My favorite food is sushi and I live in Tokyo.\")\n\n# 3. Wait for Background Processing\nprint(\"\\n[System] Consolidating memories (wait 5s)...\")\ntime.sleep(5)\n\n# 4. Recall (New Session)\n# We reset the message history here to prove it's fetching from DB, not context window\nclient.chat_history = [] \nchat_with_memory(\"Where do I live and what should I eat for dinner?\")\n</code></pre>"},{"location":"providers/ollama/","title":"Ollama: Local LLM Provider","text":""},{"location":"providers/ollama/#overview","title":"Overview","text":"<p>Ollama enables you to run LLMs locally on your machine, providing complete privacy and zero API costs. Memlayer's Ollama wrapper adds persistent memory capabilities to any Ollama-supported model.</p> <p>Key Benefits: - \u2705 Fully offline operation (no internet required) - \u2705 Complete data privacy (nothing leaves your machine) - \u2705 Zero API costs - \u2705 Fast inference on modern hardware - \u2705 Support for 100+ open-source models</p>"},{"location":"providers/ollama/#installation","title":"Installation","text":""},{"location":"providers/ollama/#1-install-ollama","title":"1. Install Ollama","text":"<p>macOS/Linux:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre> <p>Windows: Download from ollama.com/download</p> <p>Verify installation:</p> <pre><code>ollama --version\n</code></pre>"},{"location":"providers/ollama/#2-install-memlayer-with-ollama-support","title":"2. Install Memlayer with Ollama Support","text":"<pre><code>pip install memlayer ollama\n</code></pre>"},{"location":"providers/ollama/#quick-start","title":"Quick Start","text":""},{"location":"providers/ollama/#start-ollama-server","title":"Start Ollama Server","text":"<pre><code>ollama serve\n</code></pre> <p>Leave this running in a terminal. Default address: <code>http://localhost:11434</code></p>"},{"location":"providers/ollama/#pull-a-model","title":"Pull a Model","text":"<pre><code>ollama pull qwen3:14b\n\n### Basic Usage\n\n```python\nfrom memlayer.wrappers.ollama import Ollama\n\n# Initialize with local model\nclient = Ollama(\n    model=\"qwen3:14b,\n    host=\"http://localhost:11434\",\n    user_id=\"alice\",\n    operation_mode=\"local\"  # Use local embeddings too\n)\n\n# Use like any other Memlayer client\nresponse = client.chat([\n    {\"role\": \"user\", \"content\": \"My name is Alice and I work on Project Phoenix\"}\n])\nprint(response)\n\n# Later - it remembers!\nresponse = client.chat([\n    {\"role\": \"user\", \"content\": \"What project do I work on?\"}\n])\nprint(response)  # \"You work on Project Phoenix\"\n</code></pre>"},{"location":"providers/ollama/#recommended-models","title":"Recommended Models","text":"<p>Since Memlayer relies on Tool Calling and JSON Extraction for memory management, you must use models capable of instruction following.</p>"},{"location":"providers/ollama/#for-speed-2s-response","title":"For Speed (&lt; 2s response)","text":"<ul> <li>Gemma 3 (1B\u20133B, Instruct) \u2013 Extremely fast, long context, very efficient.</li> <li>Instella-3B (Instruct) \u2013 New 2025 lightweight model optimized for instruction-following.</li> <li>Mistral Small 3.1 (Efficient 24B, Instruct) \u2013 Higher params but highly optimized for low-latency inference.</li> </ul>"},{"location":"providers/ollama/#for-quality-standard","title":"For Quality (Standard)","text":"<ul> <li>Qwen 3 (32B, Instruct) \u2013 Excellent logic, tool use, and long context.</li> <li>Llama 4 (8B\u201370B, Scout/Maverick variants) \u2013 The new industry standard for local models in 2025.</li> <li>Mistral Medium 3 (~24\u201332B, Instruct) \u2013 Strong balance of performance and compute cost.</li> </ul>"},{"location":"providers/ollama/#for-best-performance","title":"For Best Performance","text":"<ul> <li>Qwen 3 (235B-A22B hybrid) \u2013 State-of-the-art reasoning with massive context windows.</li> <li>Llama 4 Behemoth (Large-scale) \u2013 High-end open model with near GPT-4.5-class capability.</li> </ul>"},{"location":"providers/ollama/#configuration","title":"Configuration","text":""},{"location":"providers/ollama/#complete-configuration-example","title":"Complete Configuration Example","text":"<pre><code>from memlayer.wrappers.ollama import Ollama\n\nclient = Ollama(\n    # Model settings\n    model=\"qwen3:14b\",\n    host=\"http://localhost:11434\",\n\n    # Memory settings\n    user_id=\"alice\",\n    operation_mode=\"local\",  # Use local embeddings\n\n    # Storage paths\n    chroma_dir=\"./chroma_db\",\n    networkx_path=\"./knowledge_graph.pkl\",\n\n    # Performance tuning\n    max_search_results=5,\n    search_tier=\"balanced\",\n    salience_threshold=0.5,\n\n    # Ollama-specific\n    temperature=0.7,\n    num_ctx=4096  # Context window size\n)\n</code></pre>"},{"location":"providers/ollama/#operation-modes-with-ollama","title":"Operation Modes with Ollama","text":"<p>Local mode (recommended):</p> <pre><code>client = Ollama(\n    model=\"qwen3:14b\",\n    operation_mode=\"local\"  # Local embeddings, fully offline\n)\n# First call: ~5-10s (loads sentence-transformer model)\n# Subsequent calls: fast\n</code></pre> <p>Online mode (hybrid):</p> <pre><code>import os\nos.environ[\"OPENAI_API_KEY\"] = \"your-key\"\n\nclient = Ollama(\n    model=\"qwen3:14b\",\n    operation_mode=\"online\"  # LLM local, embeddings via OpenAI API\n)\n# Faster startup, but requires internet for embeddings\n</code></pre> <p>Lightweight mode (fastest startup):</p> <pre><code>client = Ollama(\n    model=\"qwen3:14b\",\n    operation_mode=\"lightweight\"  # No embeddings, graph-only\n)\n# Instant startup, keyword-based search only\n</code></pre>"},{"location":"providers/ollama/#streaming-support","title":"Streaming Support","text":"<p>Ollama fully supports streaming responses:</p> <pre><code>from memlayer.wrappers.ollama import Ollama\n\nclient = Ollama(model=\"qwen3:14b\", operation_mode=\"local\")\n\n# Stream response chunks\nfor chunk in client.chat([\n    {\"role\": \"user\", \"content\": \"Tell me about quantum computing\"}\n], stream=True):\n    print(chunk, end=\"\", flush=True)\nprint()  # Newline after completion\n</code></pre> <p>Performance: - First chunk: ~1-2s (includes memory search if needed) - Chunks: 1-5 characters each (smooth streaming) - Knowledge extraction: background, doesn't block stream</p>"},{"location":"providers/ollama/#complete-offline-setup","title":"Complete Offline Setup","text":"<p>Run Memlayer entirely offline with Ollama:</p> <pre><code>from memlayer.wrappers.ollama import Ollama\n\n# Fully offline - no internet required\nclient = Ollama(\n    model=\"qwen3:14b\",\n    host=\"http://localhost:11434\",\n    operation_mode=\"local\",  # Local sentence-transformer for embeddings\n    user_id=\"alice\"\n)\n\n# Everything runs locally:\n# - LLM inference (Ollama)\n# - Embeddings (sentence-transformers)\n# - Vector search (ChromaDB)\n# - Graph storage (NetworkX)\n</code></pre> <p>First-time setup:</p> <pre><code># Pull model (one-time, requires internet)\nollama pull qwen3:14b\n\n# First Python call downloads embedding model (one-time)\n# Model: all-MiniLM-L6-v2 (~80MB)\n</code></pre> <p>After setup: Completely offline, no internet needed!</p>"},{"location":"providers/ollama/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"providers/ollama/#custom-ollama-host","title":"Custom Ollama Host","text":"<pre><code># Remote Ollama server\nclient = Ollama(\n    model=\"qwen3:14b\",\n    host=\"http://192.168.1.100:11434\",  # Remote server\n    operation_mode=\"local\"\n)\n</code></pre>"},{"location":"providers/ollama/#custom-context-window","title":"Custom Context Window","text":"<pre><code>client = Ollama(\n    model=\"qwen3:14b\",\n    num_ctx=8192,  # Increase context window (if model supports it)\n)\n</code></pre>"},{"location":"providers/ollama/#custom-temperature","title":"Custom Temperature","text":"<pre><code>client = Ollama(\n    model=\"qwen3:14b\",\n    temperature=0.3,  # Lower = more focused, higher = more creative\n)\n</code></pre>"},{"location":"providers/ollama/#custom-embedding-model","title":"Custom Embedding Model","text":"<pre><code>client = Ollama(\n    model=\"qwen3:14b\",\n    operation_mode=\"local\",\n    embedding_model=\"all-mpnet-base-v2\"  # Better quality, slower\n)\n</code></pre>"},{"location":"providers/ollama/#performance-tuning","title":"Performance Tuning","text":""},{"location":"providers/ollama/#hardware-recommendations","title":"Hardware Recommendations","text":"Model Size RAM GPU VRAM Response Time 3B (llama3.2) 8GB Optional 1-2s 7B (mistral) 16GB Optional 2-5s 8B (llama3.1) 16GB 8GB+ 2-5s 70B (llama3.1:70b) 40GB+ 24GB+ 5-15s"},{"location":"providers/ollama/#gpu-acceleration","title":"GPU Acceleration","text":"<p>Ollama automatically uses GPU if available (NVIDIA, AMD, Apple Silicon):</p> <pre><code># Verify GPU usage\nollama run llama3.2\n\n# In another terminal:\nnvidia-smi  # For NVIDIA GPUs\n# or\nrocm-smi   # For AMD GPUs\n</code></pre>"},{"location":"providers/ollama/#model-loading-time","title":"Model Loading Time","text":"<p>First inference loads model to memory (~2-5s). Keep Ollama running to avoid reload:</p> <pre><code># Keep model loaded\nollama run llama3.2\n\n# In another terminal/notebook, use Memlayer\n# Model is already in memory, responses are instant\n</code></pre>"},{"location":"providers/ollama/#concurrent-requests","title":"Concurrent Requests","text":"<p>Ollama handles concurrent requests efficiently:</p> <pre><code>import concurrent.futures\n\nclients = [Ollama(model=\"llama3.2\", user_id=f\"user{i}\") \n           for i in range(5)]\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    futures = [\n        executor.submit(c.chat, [{\"role\": \"user\", \"content\": f\"Hello {i}\"}])\n        for i, c in enumerate(clients)\n    ]\n    responses = [f.result() for f in futures]\n</code></pre>"},{"location":"providers/ollama/#troubleshooting","title":"Troubleshooting","text":""},{"location":"providers/ollama/#connection-refused-error","title":"\"Connection refused\" Error","text":"<p>Problem: Ollama server not running</p> <p>Solution:</p> <pre><code>ollama serve\n</code></pre>"},{"location":"providers/ollama/#slow-first-response","title":"Slow First Response","text":"<p>Problem: Model loading into memory</p> <p>Solution: Keep Ollama server running with model loaded:</p> <pre><code>ollama run llama3.2\n# Keep this terminal open\n</code></pre>"},{"location":"providers/ollama/#out-of-memory","title":"Out of Memory","text":"<p>Problem: Model too large for your hardware</p> <p>Solution: Use smaller model:</p> <pre><code>ollama pull llama3.2  # 3B model, needs only 8GB RAM\n</code></pre>"},{"location":"providers/ollama/#model-download-fails","title":"Model Download Fails","text":"<p>Problem: Network issues during pull</p> <p>Solution: Retry with resume:</p> <pre><code>ollama pull llama3.2  # Automatically resumes\n</code></pre>"},{"location":"providers/ollama/#embeddings-download-fails-local-mode","title":"Embeddings Download Fails (Local Mode)","text":"<p>Problem: First-time sentence-transformer download fails</p> <p>Solution: Manually download:</p> <pre><code>from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n# Now Memlayer will find cached model\n</code></pre>"},{"location":"providers/ollama/#complete-example","title":"Complete Example","text":"<pre><code>from memlayer.wrappers.ollama import Ollama\nimport time\n\n# Initialize fully offline client\nclient = Ollama(\n    model=\"qwen3:14b,\n    host=\"http://localhost:11434\",\n    operation_mode=\"local\",\n    user_id=\"alice\"\n)\n\ndef chat(message):\n    \"\"\"Send a message and stream the response.\"\"\"\n    print(f\"\\n\ud83e\udd16 Assistant: \", end=\"\", flush=True)\n    start = time.time()\n\n    for chunk in client.chat([\n        {\"role\": \"user\", \"content\": message}\n    ], stream=True):\n        print(chunk, end=\"\", flush=True)\n\n    elapsed = time.time() - start\n    print(f\"\\n\u23f1\ufe0f  Response time: {elapsed:.2f}s\\n\")\n\n# Example conversation\nprint(\"\ud83d\udc64 User: My name is Alice and I love hiking\")\nchat(\"My name is Alice and I love hiking\")\n\nprint(\"\ud83d\udc64 User: What do I like to do?\")\nchat(\"What do I like to do?\")\n\nprint(\"\ud83d\udc64 User: Plan a weekend activity for me\")\nchat(\"Plan a weekend activity for me\")\n</code></pre>"},{"location":"providers/ollama/#next-steps","title":"Next Steps","text":"<ul> <li>Basics Quickstart: General getting started guide</li> <li>Streaming Mode: Learn about streaming responses</li> <li>Operation Modes: Deep dive into local vs online modes</li> <li>Examples: Complete working code</li> <li>Ollama Docs: Official Ollama documentation</li> </ul>"},{"location":"providers/openai/","title":"OpenAI Provider Notes","text":""},{"location":"providers/openai/#credentials","title":"Credentials","text":"<ul> <li>Set <code>OPENAI_API_KEY</code> in your environment or pass <code>api_key</code> to <code>OpenAI(...)</code>.</li> </ul>"},{"location":"providers/openai/#embeddings","title":"Embeddings","text":"<ul> <li>Default online embedding model: <code>text-embedding-3-small</code>.</li> <li>For lower latency on large volumes, consider batching embedding calls in your own pipeline.</li> </ul>"},{"location":"providers/openai/#model-selection","title":"Model selection","text":"<ul> <li>Synthesized answers use <code>model</code> configured on the <code>OpenAI</code> client. Use smaller models for cheaper, faster results when appropriate.</li> </ul> <p>\u00bb See also: <code>docs/tuning/operation_mode.md</code></p>"},{"location":"services/consolidation/","title":"Consolidation Service","text":"<p>The ConsolidationService performs the heavy lifting of turning raw text into stored memories.</p>"},{"location":"services/consolidation/#responsibilities","title":"Responsibilities","text":"<ul> <li>Run salience filtering to decide which sentences to keep</li> <li>Extract facts, entities, and relationships via LLM calls (<code>analyze_and_extract_knowledge</code>)</li> <li>Store facts as vector records and entities/relationships in the graph</li> <li>Run asynchronously in background worker threads</li> </ul>"},{"location":"services/consolidation/#programmatic-usage","title":"Programmatic usage","text":"<ul> <li><code>client.consolidation_service.consolidate(text, user_id)</code> \u2014 directly call the consolidation pipeline</li> <li><code>client.update_from_text(text)</code> \u2014 convenience wrapper that calls the consolidation service</li> </ul>"},{"location":"services/consolidation/#notes","title":"Notes","text":"<ul> <li>Consolidation is non-blocking: it runs in a background thread and returns quickly.</li> <li>Ensure <code>client.close()</code> is called on shutdown so background threads stop gracefully.</li> </ul>"},{"location":"services/consolidation/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If entities appear as strings in the graph, ensure <code>analyze_and_extract_knowledge</code> returns entities as <code>[{\"name\": ..., \"type\": ...}]</code>.</li> <li>If LLM extraction fails often, check provider credentials and model availability.</li> </ul> <p>\u00bb See also: <code>docs/services/curation.md</code> and <code>docs/tuning/salience_threshold.md</code></p>"},{"location":"services/curation/","title":"Curation Service","text":"<p>The <code>CurationService</code> periodically evaluates stored memories and performs lifecycle actions:</p> <ul> <li>Archive low-relevance memories (moves them to an archived status)</li> <li>Delete expired memories (per the <code>expiration_date</code> on facts)</li> </ul>"},{"location":"services/curation/#how-it-works","title":"How it works","text":"<ul> <li>Runs every <code>curation_interval_seconds</code> (configurable)</li> <li>Uses hybrid relevance scoring (vector similarity, access recency, importance score)</li> <li>Archives when relevance &lt; <code>archive_threshold</code> (default ~0.3)</li> <li>Deletes when current time &gt; <code>expiration_date</code></li> </ul>"},{"location":"services/curation/#programmatic-control","title":"Programmatic control","text":"<ul> <li>Access via <code>client.curation_service</code> (property will start it if not running)</li> <li><code>client.curation_service.start()</code> \u2014 explicitly start</li> <li><code>client.curation_service.stop()</code> \u2014 stop cleanly</li> </ul>"},{"location":"services/curation/#practical-tips","title":"Practical tips","text":"<ul> <li>For tests, use <code>curation_interval_seconds=10</code> to speed cycles up.</li> <li>Call <code>client.close()</code> in your application exit path to ensure curation stops before storage is closed (prevents file locks).</li> </ul> <p>\u00bb See also: <code>docs/tuning/intervals.md</code></p>"},{"location":"storage/chroma/","title":"ChromaDB (Vector Storage)","text":"<p>Notes and gotchas when using ChromaDB as the vector store:</p>"},{"location":"storage/chroma/#metadata-types","title":"Metadata types","text":"<ul> <li>ChromaDB accepts only simple types in metadata: <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>.</li> <li>The code filters out <code>None</code> values before saving to Chroma. If you need <code>null</code>, store a sentinel string like <code>\"&lt;null&gt;\"</code>.</li> </ul>"},{"location":"storage/chroma/#windows-file-locks","title":"Windows file locks","text":"<ul> <li>On Windows, SQLite-based backends can keep file handles open. To avoid \"PermissionError\" during cleanup:</li> <li>Call <code>client.close()</code> to stop background services and close the vector client before deleting files.</li> <li>If you see <code>PermissionError</code>, add a short <code>time.sleep(2)</code> and <code>gc.collect()</code> before retrying.</li> </ul>"},{"location":"storage/chroma/#performance-tuning","title":"Performance tuning","text":"<ul> <li>Set <code>operation_mode=online</code> to use managed embeddings for faster startup.</li> <li>Tune <code>n_results</code> and <code>search_tier</code> to control how many vectors are retrieved.</li> </ul>"},{"location":"storage/chroma/#backup-and-restore","title":"Backup and restore","text":"<ul> <li>Chroma stores its DB files in the <code>storage_path/chroma</code> directory. Back up this folder to preserve vectors.</li> </ul> <p>\u00bb See also: <code>docs/storage/networkx.md</code></p>"},{"location":"storage/networkx/","title":"NetworkX Knowledge Graph Storage","text":""},{"location":"storage/networkx/#overview","title":"Overview","text":"<ul> <li>NetworkX is used as an in-memory graph with on-disk persistence (pickle) for the knowledge graph.</li> <li>The graph stores nodes for entities and facts, with attributes like <code>created_timestamp</code>, <code>last_accessed_timestamp</code>, <code>importance_score</code>, and <code>status</code>.</li> </ul>"},{"location":"storage/networkx/#file-format-durability","title":"File format &amp; durability","text":"<ul> <li>The graph is persisted to <code>storage_path/knowledge_graph.pkl</code> after modifications.</li> <li>On load, the code will attempt to detect corruption and back up problematic files to <code>*.pkl.corrupted</code>.</li> </ul>"},{"location":"storage/networkx/#node-schema","title":"Node schema","text":"<ul> <li>Entity node: <code>{\"name\": ..., \"type\": ..., ...}</code></li> <li>Fact node: <code>{\"fact\": ..., \"importance_score\": ..., \"expiration_date\": ...}</code></li> </ul>"},{"location":"storage/networkx/#recovery-backups","title":"Recovery &amp; backups","text":"<ul> <li>Regularly back up <code>knowledge_graph.pkl</code> for production systems.</li> <li>If the graph fails to load, the runtime will create a fresh graph and optionally back up the corrupted file.</li> </ul> <p>\u00bb See also: <code>docs/storage/chroma.md</code></p>"},{"location":"tuning/intervals/","title":"Scheduler and Curation Intervals","text":"<p>Memlayer exposes two interval knobs to control background automation:</p> <ul> <li><code>scheduler_interval_seconds</code> (default: 60)</li> <li>Controls how often the scheduler wakes up to check for due tasks/reminders.</li> <li> <p>Lower values mean more responsive reminders but higher CPU wakeups.</p> </li> <li> <p><code>curation_interval_seconds</code> (default: 3600)</p> </li> <li>Controls how often the <code>CurationService</code> evaluates memories for archiving and deletion.</li> <li>Lower values let low-importance memories be archived/deleted sooner; higher values reduce background I/O.</li> </ul>"},{"location":"tuning/intervals/#how-to-configure","title":"How to configure","text":"<pre><code>client = OpenAI(\n    scheduler_interval_seconds=30,\n    curation_interval_seconds=600\n)\n</code></pre>"},{"location":"tuning/intervals/#practical-guidance","title":"Practical guidance","text":"<ul> <li>For local development: <code>curation_interval_seconds=10</code> is useful for tests.</li> <li>For production: <code>curation_interval_seconds</code> between 900 (15 minutes) and 3600 (1 hour) is reasonable.</li> <li>If your storage backend shows file locks (Windows + Chroma), increase <code>curation_interval_seconds</code> and ensure <code>close()</code> is called during shutdown.</li> </ul>"},{"location":"tuning/intervals/#monitoring","title":"Monitoring","text":"<ul> <li>Use <code>client.last_trace</code> and logging to observe curation cycles.</li> <li>The CurationService prints debug logs when archiving/deleting memories; watch these logs to validate settings.</li> </ul> <p>\u00bb See also: <code>docs/services/curation.md</code></p>"},{"location":"tuning/operation_mode/","title":"Operation Modes: Architecture &amp; Performance Implications","text":""},{"location":"tuning/operation_mode/#overview","title":"Overview","text":"<p><code>operation_mode</code> is a fundamental architectural choice that determines how Memlayer computes embeddings for semantic search. This affects startup time, runtime performance, cost, resource usage, and deployment constraints.</p> <pre><code>from memlayer.wrappers.openai import OpenAI\n\nclient = OpenAI(\n    model=\"gpt-4.1-mini\",\n    operation_mode=\"online\"  # or \"local\" or \"lightweight\"\n)\n</code></pre>"},{"location":"tuning/operation_mode/#the-three-modes","title":"The Three Modes","text":""},{"location":"tuning/operation_mode/#1-online-mode-default","title":"1. Online Mode (Default)","text":"<p>Architecture: Uses OpenAI's API for computing text embeddings.</p> <pre><code>User Query \u2192 Vector Search (OpenAI API) \u2192 ChromaDB Lookup \u2192 Graph Traversal \u2192 Results\n             \u2514\u2500 API call (~50-100ms)\n</code></pre> <p>When to use: - \u2705 Serverless/cloud deployments (AWS Lambda, Cloud Functions) - \u2705 Production applications with internet access - \u2705 When you want minimal local resource usage - \u2705 Rapid development without model downloads</p> <p>Tradeoffs: - Startup time: Fast (~200ms, no model loading) - Search latency: 100-300ms (includes API call) - Memory usage: Low (~50MB base) - Cost: ~$0.0001 per search (API call) - Privacy: Embeddings computed by OpenAI - Reliability: Requires internet connection</p> <p>Architecture components:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Memlayer (online mode)                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u2022 ChromaStorage (vector DB)            \u2502\n\u2502  \u2022 NetworkX/Memgraph (graph DB)         \u2502\n\u2502  \u2022 OpenAI Embeddings API                \u2502\n\u2502  \u2022 Salience Gate (API embeddings)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Configuration:</p> <pre><code>client = OpenAI(\n    operation_mode=\"online\",\n    embedding_model=\"text-embedding-3-small\",  # OpenAI model\n    salience_threshold=0.5\n)\n</code></pre>"},{"location":"tuning/operation_mode/#2-local-mode","title":"2. Local Mode","text":"<p>Architecture: Uses a local sentence-transformer model for embeddings.</p> <pre><code>User Query \u2192 Vector Search (Local Model) \u2192 ChromaDB Lookup \u2192 Graph Traversal \u2192 Results\n             \u2514\u2500 GPU/CPU inference (~20-50ms)\n</code></pre> <p>When to use: - \u2705 Privacy-sensitive deployments (no external API calls) - \u2705 Offline/air-gapped environments - \u2705 High-volume applications (no per-query API cost) - \u2705 GPU-accelerated servers</p> <p>Tradeoffs: - Startup time: Slow first call (~5-10s model load, cached after) - Search latency: 50-200ms (no API dependency) - Memory usage: High (~500MB-2GB depending on model) - Cost: Zero API costs, higher compute costs - Privacy: All computation on-premises - Reliability: No internet required</p> <p>Architecture components:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Memlayer (local mode)                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u2022 ChromaStorage (vector DB)            \u2502\n\u2502  \u2022 NetworkX/Memgraph (graph DB)         \u2502\n\u2502  \u2022 sentence-transformers (local model)  \u2502\n\u2502  \u2022 Salience Gate (local embeddings)     \u2502\n\u2502  \u2022 GPU/CPU inference pipeline           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Configuration:</p> <pre><code>client = OpenAI(\n    operation_mode=\"local\",\n    embedding_model=\"all-MiniLM-L6-v2\",  # Local model\n    salience_threshold=0.5\n)\n</code></pre> <p>First-time setup:</p> <pre><code># First call downloads model (~80MB) and loads to memory\nclient.chat([{\"role\": \"user\", \"content\": \"test\"}])  # ~5-10s\n\n# Subsequent calls are fast (model cached in memory)\nclient.chat([{\"role\": \"user\", \"content\": \"hello\"}])  # ~50ms search\n</code></pre>"},{"location":"tuning/operation_mode/#3-lightweight-mode","title":"3. Lightweight Mode","text":"<p>Architecture: No embeddings, pure graph-based memory with keyword matching.</p> <pre><code>User Query \u2192 Graph Keyword Search \u2192 NetworkX Traversal \u2192 Results\n             \u2514\u2500 O(n) text scan (~10-50ms)\n</code></pre> <p>When to use: - \u2705 Demos and rapid prototyping - \u2705 Minimal resource constraints - \u2705 When semantic search isn't critical - \u2705 Testing and development</p> <p>Tradeoffs: - Startup time: Instant (~10ms) - Search latency: 10-100ms (no embeddings) - Memory usage: Minimal (~20MB base) - Cost: Zero (no API, no models) - Search quality: Lower recall, keyword-based only - Reliability: No dependencies</p> <p>Architecture components:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Memlayer (lightweight mode)            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u2022 NetworkX/Memgraph (graph DB only)    \u2502\n\u2502  \u2022 Keyword-based search                 \u2502\n\u2502  \u2022 No vector embeddings                 \u2502\n\u2502  \u2022 No salience gate                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Configuration:</p> <pre><code>client = OpenAI(\n    operation_mode=\"lightweight\",\n    # No embedding_model needed\n)\n</code></pre> <p>Search behavior:</p> <pre><code># Finds exact matches and entity relationships only\nclient.chat([{\"role\": \"user\", \"content\": \"What's my name?\"}])\n# \u2705 Works: \"My name is Alice\" stored \u2192 \"Alice\" found\n\nclient.chat([{\"role\": \"user\", \"content\": \"Tell me about my identity\"}])\n# \u274c May not work: \"identity\" doesn't match \"name\" keyword\n</code></pre>"},{"location":"tuning/operation_mode/#comparison-table","title":"Comparison Table","text":"Feature Online Local Lightweight Startup time ~200ms ~5-10s (first call) ~10ms Search latency 100-300ms 50-200ms 10-100ms Memory usage ~50MB ~500MB-2GB ~20MB API costs ~$0.0001/search $0 $0 Privacy API call to OpenAI Fully local Fully local Search quality High (semantic) High (semantic) Medium (keywords) GPU benefit None Yes (faster inference) None Internet required Yes No No Best for Production Privacy/offline Demos/testing"},{"location":"tuning/operation_mode/#architecture-deep-dive","title":"Architecture Deep Dive","text":""},{"location":"tuning/operation_mode/#storage-backend-impact","title":"Storage Backend Impact","text":"<pre><code># Online mode: Full stack\nstorage_stack = {\n    \"chroma\": ChromaStorage(),      # Vector embeddings\n    \"graph\": NetworkXStorage(),     # Entity relationships\n    \"embedder\": OpenAIEmbeddings()  # API-based\n}\n\n# Local mode: Full stack\nstorage_stack = {\n    \"chroma\": ChromaStorage(),           # Vector embeddings\n    \"graph\": NetworkXStorage(),          # Entity relationships\n    \"embedder\": SentenceTransformer()    # Local model\n}\n\n# Lightweight mode: Graph only\nstorage_stack = {\n    \"chroma\": None,                 # Disabled\n    \"graph\": NetworkXStorage(),     # Only component\n    \"embedder\": None                # Disabled\n}\n</code></pre>"},{"location":"tuning/operation_mode/#salience-gate-behavior","title":"Salience Gate Behavior","text":"<p>The salience gate filters trivial content before storing it. Behavior varies by mode:</p> <p>Online mode:</p> <pre><code># Uses OpenAI embeddings API to compute similarity\n# Cache: ~/.memlayer_cache/salience_prototypes_online.pkl\n# First call: ~1-2s (compute 100 prototype embeddings)\n# Cached calls: ~0.01s (load from disk)\n</code></pre> <p>Local mode:</p> <pre><code># Uses local sentence-transformer model\n# Cache: ~/.memlayer_cache/salience_prototypes_local.pkl\n# First call: ~5-10s (load model + compute prototypes)\n# Cached calls: ~0.05s (local inference)\n</code></pre> <p>Lightweight mode:</p> <pre><code># Salience gate disabled entirely\n# All content stored (no filtering)\n# Zero overhead\n</code></pre>"},{"location":"tuning/operation_mode/#memory-search-flow","title":"Memory Search Flow","text":"<p>Online mode search:</p> <pre><code>1. User query: \"What's my name?\"\n2. Embed query \u2192 OpenAI API call (~50ms)\n3. ChromaDB vector search (~30ms)\n4. NetworkX graph traversal (~20ms)\n5. Return results (~100ms total)\n</code></pre> <p>Local mode search:</p> <pre><code>1. User query: \"What's my name?\"\n2. Embed query \u2192 Local inference (~20ms)\n3. ChromaDB vector search (~30ms)\n4. NetworkX graph traversal (~20ms)\n5. Return results (~70ms total)\n</code></pre> <p>Lightweight mode search:</p> <pre><code>1. User query: \"What's my name?\"\n2. Keyword extraction (~5ms)\n3. NetworkX keyword search (~30ms)\n4. Graph traversal (~20ms)\n5. Return results (~55ms total)\n</code></pre>"},{"location":"tuning/operation_mode/#choosing-the-right-mode","title":"Choosing the Right Mode","text":""},{"location":"tuning/operation_mode/#decision-tree","title":"Decision Tree","text":"<pre><code>Need privacy/offline?\n\u251c\u2500 Yes \u2192 LOCAL mode\n\u2514\u2500 No \u2192 Need semantic search?\n         \u251c\u2500 Yes \u2192 ONLINE mode (recommended)\n         \u2514\u2500 No \u2192 LIGHTWEIGHT mode\n</code></pre>"},{"location":"tuning/operation_mode/#by-use-case","title":"By Use Case","text":"<p>Serverless Functions (Lambda, Cloud Functions)</p> <pre><code># Use online mode - fast cold starts\nclient = OpenAI(operation_mode=\"online\")\n</code></pre> <p>Long-running Servers</p> <pre><code># Use local mode - absorb startup cost once\nclient = OpenAI(operation_mode=\"local\")\n# First request pays model load cost\n# All subsequent requests are fast\n</code></pre> <p>Demos and Prototypes</p> <pre><code># Use lightweight - instant startup\nclient = OpenAI(operation_mode=\"lightweight\")\n</code></pre> <p>Privacy-Sensitive Applications</p> <pre><code># Use local mode - no external API calls\nclient = OpenAI(operation_mode=\"local\")\n</code></pre> <p>High-Volume Production</p> <pre><code># Use local mode on GPU server - no API costs\nclient = OpenAI(\n    operation_mode=\"local\",\n    embedding_model=\"all-mpnet-base-v2\"  # Better quality\n)\n</code></pre>"},{"location":"tuning/operation_mode/#performance-tuning","title":"Performance Tuning","text":""},{"location":"tuning/operation_mode/#optimizing-online-mode","title":"Optimizing Online Mode","text":"<pre><code>client = OpenAI(\n    operation_mode=\"online\",\n    embedding_model=\"text-embedding-3-small\",  # Faster than 3-large\n    max_search_results=5,                      # Fewer results = faster\n    search_tier=\"fast\"                          # Vector-only search\n)\n</code></pre>"},{"location":"tuning/operation_mode/#optimizing-local-mode","title":"Optimizing Local Mode","text":"<pre><code>client = OpenAI(\n    operation_mode=\"local\",\n    embedding_model=\"all-MiniLM-L6-v2\",  # Smaller, faster model\n    # Use GPU if available (detected automatically)\n)\n\n# Warm up model on startup\nclient.chat([{\"role\": \"user\", \"content\": \"warmup\"}])\n</code></pre>"},{"location":"tuning/operation_mode/#optimizing-lightweight-mode","title":"Optimizing Lightweight Mode","text":"<pre><code>client = OpenAI(\n    operation_mode=\"lightweight\",\n    max_search_results=3,      # Faster graph traversal\n    search_tier=\"fast\"          # Minimal graph hops\n)\n</code></pre>"},{"location":"tuning/operation_mode/#migration-between-modes","title":"Migration Between Modes","text":"<p>Switching modes requires re-embedding existing memories:</p> <pre><code># Start with online mode\nclient_online = OpenAI(\n    user_id=\"alice\",\n    operation_mode=\"online\",\n    chroma_dir=\"./chroma_online\"\n)\n\n# Migrate to local mode\nclient_local = OpenAI(\n    user_id=\"alice\", \n    operation_mode=\"local\",\n    chroma_dir=\"./chroma_local\"  # Different directory\n)\n\n# Graph data (networkx_path) can be reused\n# Vector embeddings must be recomputed\n</code></pre> <p>Note: Graph storage (NetworkX/Memgraph) is mode-independent and can be reused.</p>"},{"location":"tuning/operation_mode/#examples","title":"Examples","text":"<p>See complete examples: - Online mode: <code>examples/05_providers/openai_example.py</code> - Local mode: <code>examples/05_providers/ollama_example.py</code> - Lightweight mode: <code>examples/01_basics/getting_started.py</code></p>"},{"location":"tuning/operation_mode/#related-documentation","title":"Related Documentation","text":"<ul> <li>Basics Overview: Architecture fundamentals</li> <li>Quickstart: Getting started guide</li> <li>Salience Threshold: Content filtering</li> <li>ChromaDB Storage: Vector storage details</li> <li>Ollama Provider: Local model setup</li> </ul>"},{"location":"tuning/salience_threshold/","title":"Salience Threshold","text":"<p><code>salience_threshold</code> controls how permissive the consolidation pipeline is when deciding whether a fact should become a stored memory.</p> <ul> <li>Lower (negative) values are permissive and save more content (useful for aggressive recall).</li> <li>Higher (positive) values are strict and save less content to minimize storage and noise.</li> </ul> <p>Default: <code>0.0</code> (balanced)</p>"},{"location":"tuning/salience_threshold/#how-to-set","title":"How to set","text":"<pre><code>client = OpenAI(salience_threshold=-0.1)  # more permissive\n</code></pre>"},{"location":"tuning/salience_threshold/#guidance","title":"Guidance","text":"<ul> <li>For personal assistants where recall breadth is important, set <code>salience_threshold</code> to <code>-0.1</code> or lower.</li> <li>For privacy-sensitive or noisy inputs, set <code>salience_threshold</code> to <code>0.1</code> or higher.</li> <li>Combine with <code>operation_mode</code> to tune both accuracy and cost.</li> </ul> <p>\u00bb See also: <code>docs/tuning/operation_mode.md</code></p>"}]}